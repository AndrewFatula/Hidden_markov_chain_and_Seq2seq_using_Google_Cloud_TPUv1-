{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chatbot.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewFatula/Seq2seq_model_using_Google_Cloud_TPU/blob/master/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSA44oSsjA9t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.translate import bleu_score\n",
        "from keras import regularizers\n",
        "from copy import deepcopy as dc\n",
        "from sklearn.metrics.pairwise import cosine_similarity as cs\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZL1s4sjp1Pr",
        "colab_type": "text"
      },
      "source": [
        "Importing all needed packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-lPrdrosYJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade auth\n",
        "!pip uninstall grpcio\n",
        "!pip uninstall tensorflow\n",
        "!pip install grpcio==1.24.3\n",
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "akm2-QDop4iB",
        "colab_type": "text"
      },
      "source": [
        "Default version of tensorflow in colab is 1.15, so in order to gain all the benefits of tensorflow 2.0, current version of tensorflow needs to be uninstalled and then we can install the 2.0 version, but in order to use google cloud TPU v1 with tensorflow 2.0 before installing 2.0 version of tensorflow packages like grpcio and auth shuold be reinstalled the same way to versions specified above in code.\n",
        "\n",
        "When given packages are reinstalled runstime shuold be reset in order to activates updates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYwxehgbjLPZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1yjv0XyqADP",
        "colab_type": "text"
      },
      "source": [
        "Importing all needed tools and authentificating in google account in order to import data in colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZUB7Nxejb9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':'1Z7odQLtZ7RmWERaRDJHDvbX_3VJ2ptvu'}) # all phrases in movies\n",
        "downloaded.GetContentFile('movie_lines.txt') \n",
        "downloaded = drive.CreateFile({'id':'1TdbFyBvMGV_N8iszqTIAb7V1Vx_eSdMr'}) # all conversations in movies\n",
        "downloaded.GetContentFile('movie_conversations.txt') \n",
        "downloaded = drive.CreateFile({'id':'1YTQeB3x_HTeEA5sjrleJD3notEKaPBnM'}) # all titles of movies\n",
        "downloaded.GetContentFile('movie_titles_metadata.txt')\n",
        "downloaded = drive.CreateFile({'id':'16Lxkrsd8HV9j9IrueYXQwY0TMbSttx5P'}) # GLOVE 50-demensional pretrained vector representations for 400000 English words trained on data from Twitter \n",
        "downloaded.GetContentFile('glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C3XtnW2rqHXF",
        "colab_type": "text"
      },
      "source": [
        "Loading all needed datafiles for constucting training dataset including GLOVE 50-demensional pretrained vector representations for 400000 English words trained on data from Twitter - which is needed in training seq2seq model to check phrases similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kr9MqFWSsiyX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address iis', tpu_address)\n",
        "\n",
        "\n",
        "cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
        "    tpu=tpu_address)\n",
        "tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n",
        "\n",
        "print ('Number of devices: {}'.format(tpu_strategy.num_replicas_in_sync))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Tc7uxwurCpd",
        "colab_type": "text"
      },
      "source": [
        "Setting up TPU v1 hardware acceleration system. \n",
        "Definig TPU cluster_resolver and tpu_distributed_strategy for training seq2seq."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkngi7vtjngT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "SEPARATOR = \"+++$+++\"\n",
        "MAX_TOKENS = 10\n",
        "\n",
        "emb_file = 'glove.6B.50d.txt'\n",
        "\n",
        "def tokenize(str_):\n",
        "\treturn TweetTokenizer(preserve_case=False).tokenize(str_)\n",
        "\n",
        "\n",
        "def remove_all(str, substrings):\n",
        "  index = 0\n",
        "  for substr in substrings:\n",
        "    length = len(substr)\n",
        "    while str.find(substr) != -1:\n",
        "      index = str.find(str)\n",
        "      str = str[0:index] + str[index+length:]\n",
        "  return str\n",
        "\n",
        "def load_movies(genres):\n",
        "\n",
        "\t'''This function loads all the movie titles of specicfied genres from <movie_titles_metadata.txt> file and returns list of movie titles'''\n",
        "\n",
        "\tmovies = []\n",
        "\twith open(\"movie_titles_metadata.txt\", 'rb') as gf:\n",
        "\n",
        "\t\tfor line in gf:\n",
        "\t\t\tline = str(line, encoding='utf-8', errors='ignore')\n",
        "\t\t\tarr_line = list(map( lambda x: x.strip(), line.split(SEPARATOR) ))\n",
        "\t\t\tline_genres = list(map( lambda x: x.strip(\" '\"), arr_line[-1].strip(\"[]\").split(\",\") ))\n",
        "\n",
        "\t\t\tfor genre in genres:\n",
        "\t\t\t\tif genre in line_genres:\n",
        "\t\t\t\t\tmovies.append(arr_line[0])\n",
        "\t\t\t\t\tbreak\n",
        "\n",
        "\treturn movies\t\t\t\t\n",
        "\n",
        "\n",
        "def read_phrases(movies = []):\n",
        "\n",
        "\t''' This function loads all phrasses from <movie_lines.txt> file which are said in given movie list as an argument of this function\n",
        "\t\t\tIf function is called with no argument than it reads phrasses from all the movies available in dataset  '''\n",
        "\n",
        "\tphrases = {}\n",
        "\n",
        "\twith open('movie_lines.txt', 'rb') as lf:\n",
        "\n",
        "\t\tfor line in lf:\n",
        "\t\t\tline = str(line, encoding='utf-8', errors='ignore').replace(\"<u>\",\"\").replace(\"</u>\",\"\")\n",
        "\n",
        "\t\t\tarr_line = list(map( lambda x: x.strip(), line.split(SEPARATOR) ))\n",
        "\t\t\t#phrases are loaded in dictionary, the key is index of a conversation\n",
        "\t\t\tif not movies:\n",
        "\t\t\t\tphrases[arr_line[0]] = tokenize(remove_all(arr_line[-1],[\"<b>\",\"</b>\",\"<u>\",\"</u>\",\"<i>\",\"</i>\",\"<u>\", \"</u>\"]))\n",
        "\t\t\telif arr_line[2] in movies: \n",
        "\t\t\t\tphrases[arr_line[0]] = tokenize(remove_all(arr_line[-1],[\"<b>\",\"</b>\",\"<u>\",\"</u>\",\"<i>\",\"</i>\",\"<u>\", \"</u>\"]))\n",
        "\t\t\t\t\n",
        "\treturn phrases\t\n",
        "\t\n",
        "\n",
        "\n",
        "def read_dialogues(phrases, movies):\n",
        "\n",
        "\t''' this function constructs dialogs from phrases in the movies,\n",
        "\t\t\tall the needed information for dialogs construction is readed from file <movie_conversations.txt>'''\n",
        "\n",
        "\tdialogues = []\n",
        "\twith open(\"movie_conversations.txt\", 'rb') as df:\n",
        "\n",
        "\t\tfor line in df:\t\n",
        "\t\t\tline = str(line, encoding='utf-8', errors='ignore')\n",
        "\t\t\tarr_line = list(map( lambda x: x.strip(), line.split(SEPARATOR) ))\n",
        "\t\t\tdialog = list(map( lambda x: x.strip(\"' \"), arr_line[-1].strip(\"[]\").split(\",\") ))\n",
        "\n",
        "\t\t\tif not movies:\n",
        "\t\t\t\tdialogues.append([phrases[phrase] for phrase in dialog])\n",
        "\t\t\telif arr_line[2] in movies:\t\n",
        "\t\t\t\tdialogues.append([phrases[phrase] for phrase in dialog])\n",
        "\n",
        "\treturn dialogues\t\t\t\t\n",
        "\n",
        "\n",
        "def get_phrase_pairs(genres = None, max_tokins = MAX_TOKENS, n_pairs = None):\n",
        "\n",
        "\t''' This function constructs phrase_pairs dataset where each instance of dataset is phrase and response to that phrase '''\n",
        "\n",
        "\t#when genres in not specified it read phrses from all the movies is available\n",
        "\tif genres == None:\n",
        "\t\tall_phrases = read_phrases()\n",
        "\t\tmovies = []\n",
        "\telse:\n",
        "\t\tmovies = load_movies(genres)\n",
        "\t\tall_phrases = read_phrases(movies)\t\n",
        "\t\n",
        "\t#before constructing phrase pairs dataset we need to get conversations dataset from all readed phrases\n",
        "\tconversations = read_dialogues(all_phrases, movies)\n",
        "\tphrase_pairs = []\n",
        "\n",
        "\n",
        "\tfor conv in conversations:\n",
        "\t\tprev_phrase = None\n",
        "\n",
        "\t\tfor phrase in conv:\n",
        "\t\t\tif prev_phrase is not None and (max_tokins == None or (len(phrase) <= max_tokins and len(prev_phrase) <= max_tokins)):\n",
        "\t\t\t\tphrase_pairs.append((prev_phrase, phrase))\n",
        "\t\t\tprev_phrase = phrase\n",
        "\n",
        "\tif (n_pairs == None) or (n_pairs >= len(phrase_pairs)) :\n",
        "\t\treturn phrase_pairs, conversations\n",
        "\telse:\n",
        "\t\tnp.random.shuffle(phrase_pairs)\n",
        "\t\treturn phrase_pairs[0:n_pairs], conversations"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hzsni4HrHzg",
        "colab_type": "text"
      },
      "source": [
        "Fuctions above are used to construct phrase_pairs dataset from movies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8z2JGdKMj7MS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "MIN_TOKEN_FREQ = 10\n",
        "EMBEDDING_SIZE = 64\n",
        "HIDDEN_SIZE = 1024\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "\n",
        "\n",
        "def read_embeddings(filepath):\n",
        "\n",
        "\t'''this function reads pretrained on twitter data word2vec embeddings from embedings file, \n",
        "\t\t it returns list of vector representations of each word in embeddings file and token words dictionary'''\n",
        "\n",
        "\twords = []\n",
        "\twords.append(\"BEGIN\")\n",
        "\twords.append(\"END\")\n",
        "\twords_dict = {}\n",
        "\trepresentations = []\n",
        "\trepresentations.append([0]*50)\n",
        "\trepresentations.append([1]*50)\n",
        "\n",
        "\twith open(filepath) as fp:\n",
        "\t\tline = fp.readline()\n",
        "\t\twhile line:\n",
        "\t\t\tline_content = line.split(\" \")\n",
        "\t\t\twords.append(line_content[0])\n",
        "\t\t\trepresentations.append(list(map(lambda x: float(x), line_content[1:])))\n",
        "\t\t\tline = fp.readline()\n",
        "\t \n",
        "\tfor i in range(len(words)-1):\n",
        "\t\twords_dict[words[i]] = i\n",
        "\n",
        "\treturn np.array(representations[:-1]), words_dict\n",
        "\n",
        "\n",
        "def word_corrector(word):\n",
        "\n",
        "\t''' this function is used to convert commonly used english words shortcuts to corresponding them full forms '''\n",
        "\n",
        "\tdots = ['. ..', '. . .', '..', '. ...','...  ...', '.  ...', '. .']\n",
        "\tcomonly_used = [\"that\", \"what\", \"there\", \"who\", \"where\", \"how\"]\n",
        "\tpronouns = [\"it\", \"she\", \"he\"]\n",
        "\n",
        "\tif word[-3:] == \"'ll\":\n",
        "\t\tdecoded = \"+$+\" + word[:-3] + \"@\" + \"will\"\n",
        "\t\treturn decoded\n",
        "\n",
        "\telif word[-2:] == \"'d\":\n",
        "\t\tdecoded = \"+$+\" + word[:-2] + \"@\" + \"would\"\n",
        "\t\treturn decoded \n",
        "\n",
        "\telif word[-2:] == \"'s\" and word[:-2] in comonly_used+pronouns:\n",
        "\t\tdecoded = \"+$+\" + word[:-2] + \"@\" + \"is\"\n",
        "\t\treturn decoded\n",
        "\n",
        "\telif word[-2:] == \"'s\" and word[:-2] and not word[:-2] in comonly_used+pronouns:\n",
        "\t\tdecoded = \"+$+\" + word[:-2] + \"@\" + \"'s\"\n",
        "\t\treturn decoded\n",
        "\n",
        "\telif word[-2:] == \"'t\":\n",
        "\t\tif word == \"can't\":\n",
        "\t\t\tdecoded = \"+$+\" + \"can\" + \"@\" + \"not\"\n",
        "\t\telse:\n",
        "\t\t\tdecoded = \"+$+\" + word[:-3] + \"@\" + \"not\"\t\n",
        "\t\treturn decoded   \n",
        "\n",
        "\telif word[-3:] == \"'re\":\n",
        "\t\tdecoded = \"+$+\" + word[:-3] + \"@\" + \"are\"\n",
        "\t\treturn decoded \n",
        "\n",
        "\telif word == \"i'm\":\n",
        "\t\tdecoded = \"+$+\" + \"i\" + \"@\" + \"am\"\n",
        "\t\treturn decoded \n",
        "\n",
        "\telif word[-3:] == \"'ve\":\n",
        "\t\tdecoded = \"+$+\" + word[:-3] + \"@\" + \"have\"\n",
        "\t\treturn decoded \n",
        "\n",
        "\telif word[:2] == \"y'\":\n",
        "\t\tdecoded = \"+$+\" + \"you\" + \"@\" + word[2:]\n",
        "\t\treturn decoded\n",
        "\n",
        "\telif word[:2] == \"dont\":\n",
        "\t\tdecoded = \"+$+\" + \"do\" + \"@\" + \"not\"\n",
        "\t\treturn decoded\n",
        "\n",
        "\telse:\n",
        "\t\tif word in dots:\n",
        "\t\t\tword = \"...\"\n",
        "\t\tif word == \"u\":\n",
        "\t\t\tword = \"you\"\n",
        "\t\tif word == \"ur\":\n",
        "\t\t\tword = \"your\"\t\n",
        "\t\treturn word\n",
        "\n",
        "\n",
        "####### \n",
        "def get_word_dict2(dialogues, emb_dict):\n",
        "\n",
        "\t''' this function constructs token dictionary with words which is available in embeddings dictionary retrieved from GLOVE 50-dimensional wordvectors representations,\n",
        "\t\t\tand is present in phrase_pairs dataset with frequency >= than specified MIN_TOKEN_FREQ '''\n",
        "\n",
        "\tfreq_count = Counter()\n",
        "\tsizes_x = []\n",
        "\tsizes_y = []\n",
        "\n",
        "\tfor dial in dialogues:\n",
        "\t\tfor phrase in dial:\n",
        "\t\t\tfreq_count.update(phrase)\n",
        "\n",
        "\tword_set = list(map(lambda x: '+' + x[0] if x[1] >= MIN_TOKEN_FREQ else '-' + x[0], freq_count.items() ))\n",
        "\tword_dict = {\"BEGIN\":0, \"END\":1}\n",
        "\temb_indices = [0,1]\n",
        "\ti = 2\n",
        "\n",
        "\n",
        "\tfor word in word_set:\n",
        "\t\tif word[0] == \"+\":\n",
        "\t\t\tcorrect_word = word_corrector(word[1:])\n",
        "\n",
        "\t\t\tif correct_word[:3] == \"+$+\":\n",
        "\t\t\t\tcorrected1, corrected2 = correct_word[3:].split(\"@\")\n",
        "\t\t\n",
        "\t\t\t\tif corrected2 == \"'s\" and not corrected2[0] in word_dict.keys() and not corrected2[-1] in word_dict.keys():\n",
        "\t\t\t\t\tword_dict[\"'\"] = i\n",
        "\t\t\t\t\tword_dict[\"s\"] = i+1\n",
        "\t\t\t\t\temb_indices.append(emb_dict[\"'\"])\n",
        "\t\t\t\t\temb_indices.append(emb_dict[\"s\"])\n",
        "\t\t\t\t\ti+=2\n",
        "\t\t \n",
        "\t\t\t\tif not corrected1 in word_dict.keys() and corrected1 in emb_dict.keys():\n",
        "\t\t\t\t\tword_dict[corrected1] = i\n",
        "\t\t\t\t\temb_indices.append(emb_dict[corrected1])\n",
        "\t\t\t\t\ti+=1\n",
        "\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\tif corrected1 in emb_dict.keys() and not corrected1 in word_dict.keys() :\n",
        "\t\t\t\t\tword_dict[corrected1] = i\n",
        "\t\t\t\t\temb_indices.append(emb_dict[corrected1])\n",
        "\t\t\t\t\ti+=1\n",
        "\n",
        "\n",
        "\t\t\t\tif corrected2 in emb_dict.keys() and not corrected2 in word_dict.keys() :\n",
        "\t\t\t\t\tword_dict[corrected2] = i\n",
        "\t\t\t\t\temb_indices.append(emb_dict[corrected2])\n",
        "\t\t\t\t\ti+=1\n",
        "\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\telse :\n",
        "\t\t\t\tif correct_word in emb_dict.keys() and not correct_word in word_dict.keys():\n",
        "\t\t\t\t\tword_dict[correct_word] = i\n",
        "\t\t\t\t\temb_indices.append(emb_dict[correct_word])\n",
        "\t\t\t\t\ti+=1\n",
        "   \n",
        "\n",
        "\treturn word_dict, emb_indices\n",
        "\n",
        "\n",
        "\n",
        "def convert_phrases2(phrase_pairs, word_dict):\n",
        "\n",
        "\t''' this function converts all the words in phrases dataset to tokens based on token dictionary retrieved from embeddings file and phrase_pairs dataset,\n",
        "\t\t\tphrases with words which frequencies are < MIN_TOKEN_FREQ or with words which are unavailable in words embeddings file are ignored,\n",
        "\t\t\talso it converts common words shortcuts in phrases to corresponding them full forms, \n",
        "\t\t\tit returns converted to tokens phrase_pairs separately and lengths for each phrase '''\n",
        "\n",
        "\tsizes_x = []\n",
        "\tsizes_y = []\n",
        "\tnot_available = []\n",
        "\tconverted_x = []\n",
        "\tconverted_y = []\n",
        "\n",
        "\tfor pair in phrase_pairs:\n",
        "\t\tif len(pair[0]) < MAX_TOKENS+1 and len(pair[1]) < 8:\n",
        "\t\t\tphrase1 = []\n",
        "\t\t\tphrase2 = []\n",
        "\n",
        "\t\t\tfor word in pair[0]:\n",
        "\t\t\t\tcorrect_word = word_corrector(word)\n",
        "\t\t\t\tif \"<u>\" in correct_word or \"</u>\" in correct_word:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\n",
        "\t\t\t\tif \"+$+\" in correct_word:\n",
        "\t\t\t\t\tcorrected1, corrected2 = correct_word[3:].split(\"@\")\n",
        "\t\t \n",
        "\t\t\t\t\tif corrected2 == \"'s\":\n",
        "\t\t\t\t\t\tif corrected1 in word_dict.keys():\n",
        "\t\t\t\t\t\t\tphrase1.append(word_dict[corrected1])\n",
        "\t\t\t\t\t\t\tphrase1.append(word_dict[\"'\"])\n",
        "\t\t\t\t\t\t\tphrase1.append(word_dict[\"s\"])\n",
        "\t\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\tphrase1.append(word_dict[\"BEGIN\"])\n",
        "\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\tif corrected1 in word_dict.keys() and corrected2 in word_dict.keys():\n",
        "\t\t\t\t\t\tphrase1.append(word_dict[corrected1])\n",
        "\t\t\t\t\t\tphrase1.append(word_dict[corrected2])\n",
        "\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tphrase1.append(word_dict[\"BEGIN\"])\n",
        "\t\t\t\t\t\tcontinue\n",
        "\t\t\t\n",
        "\t\t\t\telse :\n",
        "\t\t\t\t\tif correct_word in word_dict.keys():\n",
        "\t\t\t\t\t\tif correct_word == \"'s\":\n",
        "\t\t\t\t\t\t\tphrase1.append(word_dict[\"'\"])\n",
        "\t\t\t\t\t\t\tphrase1.append(word_dict[\"s\"])\n",
        "\t\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\t\tphrase1.append(word_dict[correct_word])\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tphrase1.append(word_dict[\"BEGIN\"])\n",
        "\n",
        "\t\t\tfor word in pair[1]:\n",
        "\t\t\t\tcorrect_word = word_corrector(word)\n",
        "\t\t\n",
        "\t\t\t\tif \"+$+\" in correct_word:\n",
        "\t\t\t\t\tcorrected1, corrected2 = correct_word[3:].split(\"@\")\n",
        "\t\t \n",
        "\t\t\t\t\tif corrected2 == \"'s\":\n",
        "\t\t\t\t\t\tif corrected1 in word_dict.keys():\n",
        "\t\t\t\t\t\t\tphrase2.append(word_dict[corrected1])\n",
        "\t\t\t\t\t\t\tphrase2.append(word_dict[\"'\"])\n",
        "\t\t\t\t\t\t\tphrase2.append(word_dict[\"s\"])\n",
        "\t\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\tphrase2.append(word_dict[\"BEGIN\"])\n",
        "\t\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\t\tif corrected1 in word_dict.keys() and corrected2 in word_dict.keys():\n",
        "\t\t\t\t\t\tphrase2.append(word_dict[corrected1])\n",
        "\t\t\t\t\t\tphrase2.append(word_dict[corrected2])\n",
        "\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tphrase2.append(word_dict[\"BEGIN\"])\n",
        "\t\t\t\t\t\tcontinue\n",
        "\n",
        "\t\t\t\telse :\n",
        "\t\t\t\t\tif correct_word in word_dict.keys():\n",
        "\t\t\t\t\t\tif correct_word == \"'s\":\n",
        "\t\t\t\t\t\t\tphrase2.append(word_dict[\"'\"])\n",
        "\t\t\t\t\t\t\tphrase2.append(word_dict[\"s\"])\n",
        "\t\t\t\t\t\t\tcontinue\n",
        "\t\t\t\t\t\tphrase2.append(word_dict[correct_word])\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tphrase2.append(word_dict[\"BEGIN\"])\n",
        "\n",
        "\t\t\tphrase1 = [0] + phrase1 + [1]\n",
        "\t\t\tphrase2 = [0] + phrase2 + [1]\n",
        "\t\t\n",
        "\t\t\tif not word_dict[\"BEGIN\"] in phrase1[1:] and not word_dict[\"BEGIN\"] in phrase2[1:]:\n",
        "\t\t\t\tconverted_x.append(phrase1)\n",
        "\t\t\t\tconverted_y.append(phrase2)\n",
        "\n",
        "\t\t\t\tsizes_x.append(len(phrase1))\n",
        "\t\t\t\tsizes_y.append(len(phrase2))\n",
        "\t\n",
        "\tlength = len(sizes_x)\n",
        "\tsizes_x = np.array(sizes_x)\n",
        "\tsizes_y = np.array(sizes_y)\n",
        "\n",
        "\treturn converted_x, converted_y, sizes_x.astype(np.int32), sizes_y.astype(np.int32), not_available\n",
        "\n",
        "def make_addition(phrases):\n",
        "\tphrases.append(([\"hi\",\"!\"],[\"hello\"]))\n",
        "\tphrases.append(([\"hi\"],[\"hello\", \"!\"]))\n",
        "\tphrases.append(([\"hi\"],[\"hello\", \"!\"]))\n",
        "\tphrases.append(([\"hello\",\"!\"],[\"hi\", \"!\"]))\n",
        "\tphrases.append(([\"hi\",\".\"],[\"hello\", \",\",\"nice\", \"to\", \"meet\",\"you\"]))\n",
        "\tphrases.append(([\"hi\",\".\"],[\"hi\",\"good\",\"to\",\"meet\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hi\",\"!\"],[\"hello\"]))\n",
        "\tphrases.append(([\"hi\"],[\"hello\", \"!\"]))\n",
        "\tphrases.append(([\"hi\"],[\"hello\", \"!\"]))\n",
        "\tphrases.append(([\"hello\",\"!\"],[\"hi\", \"!\"]))\n",
        "\tphrases.append(([\"hi\",\"!\"],[\"hello\"]))\n",
        "\tphrases.append(([\"hi\"],[\"hello\", \"!\"]))\n",
        "\tphrases.append(([\"hi\"],[\"hello\", \"!\"]))\n",
        "\tphrases.append(([\"hello\",\"!\"],[\"hi\", \"!\"]))\n",
        "\tphrases.append(([\"hi\",\".\"],[\"hello\", \",\",\"nice\", \"to\", \"meet\",\"you\"]))\n",
        "\tphrases.append(([\"hi\",\".\"],[\"hi\",\"good\",\"to\",\"meet\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hi\",\"!\"],[\"hello\"]))\n",
        "\tphrases.append(([\"hi\"],[\"hello\", \"!\"]))\n",
        "\tphrases.append(([\"hi\"],[\"hello\", \"!\"]))\n",
        "\tphrases.append(([\"hello\",\"!\"],[\"hi\", \"!\"]))\n",
        "\tphrases.append(([\"hello\"],[\"it\",\"is\",\"my\",\"pleasure\"]))\n",
        "\tphrases.append(([\"hi\",\".\"],[\"hello\", \",\",\"nice\", \"to\", \"meet\",\"you\"]))\n",
        "\tphrases.append(([\"hi\",\"!\"],[\"hi\",\"good\",\"to\",\"meet\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hi\",\",\", \"how\", \"are\", \"you\",\"?\"],[\"i\",\"am\",\"fine\",\",\",\"thank\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hello\",\",\", \"how\", \"are\", \"you\",\"?\"],[\"i\",\"am\",\"great\",\",\",\"thank\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hi\",\"!\", \"how\", \"are\", \"you\",\"?\"],[\"not\",\"bad\",\",\",\"thank\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hello\", \"!\", \"how\", \"are\", \"you\",\"?\"],[\"i\",\"am\",\"fine\",\"thank\",\"you\",\".\"]))\n",
        "\tphrases.append(([\"hi\",\",\", \"how\", \"are\", \"you\",\"?\"],[\"i\",\"am\",\"good\",\",\",\"thank\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hello\",\",\", \"how\", \"are\", \"you\",\"?\"],[\"i\",\"am\",\"good\",\",\",\"thank\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hello\", \"!\", \"how\", \"are\", \"you\",\"?\"],[\"good\",\"enough\",\",\",\"thank\",\"you\",\"!\"]))\n",
        "\n",
        "\treturn phrases\n",
        "\n",
        "\n",
        "def calc_bleu_score(out, target):\n",
        "\tsf = bleu_score.SjmoothingFunction()\n",
        "\treturn bleu_score.sentence_bleu(target, out, smoothing_function=sf.method1, weights=(0.5, 0.5))\n",
        "\n",
        "\n",
        "def get_words_from_toukens(sentence, token_dict):\n",
        "\treturn [token_dict[value] for value in sentence]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw_btKLhrNFt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "functions above are used to prepare and preprocces phrase_pairs dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epaSlgGMM6Hp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "print(\"constructing vectorspace for pharse pairs dataset...\")\n",
        "\n",
        "all_representations, emb_token_dict = read_embeddings(emb_file)\n",
        "phrase_pairs, dialogues = get_phrase_pairs(genres = [\"comedy\", \"thriller\", \"drama\", \"crime\", \"sci-fi\", \"western\", \"fantasy\", \"mystery\", \"animation\" ,\"history\" ,\"war\"])\n",
        "phrase_pairs = make_addition(phrase_pairs)\n",
        "word_dict, transfer_indices = get_word_dict2(dialogues, emb_token_dict)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "embeddings = []\n",
        "for index in transfer_indices:\n",
        "  embeddings.append(all_representations[index])\n",
        "embeddings = np.array(embeddings)\n",
        "\n",
        "phrase_number = len(phrase_pairs)\n",
        "inverse_word_dict = {}\n",
        "\n",
        "for key in word_dict.keys():\n",
        "  inverse_word_dict[word_dict[key]] = key\n",
        "\n",
        "  \n",
        "\n",
        "print(\"vectorspace is constructed\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDQgAMsMrUix",
        "colab_type": "text"
      },
      "source": [
        "Code above constructs vectorspace for words which are present in phrase pairs dataset based on GLOVE 50-d embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vX0Z_B6KOK-I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"preparing and preprocces dataset...\")\n",
        "\n",
        "\n",
        "converted_x, converted_y, true_lengths_x, true_lengths_y, not_available = convert_phrases2(phrase_pairs, word_dict)\n",
        "\n",
        "width_x = int(max(true_lengths_x))\n",
        "width_y = int(max(true_lengths_y))\n",
        "\n",
        "sparse_x = np.zeros((len(true_lengths_x), width_x))\n",
        "sparse_y = np.zeros((len(true_lengths_x), width_y))\n",
        "semi_hot_y = np.zeros((len(true_lengths_x), width_y))\n",
        "\n",
        "\n",
        "for i in range(len(true_lengths_x)):\n",
        "  sparse_x[i, :true_lengths_x[i]] = np.array(converted_x[i])\n",
        "  sparse_y[i, :true_lengths_y[i]] = np.array(converted_y[i])\n",
        "  semi_hot_y[i, : true_lengths_y[i]] = np.ones((true_lengths_y[i]))\n",
        "\n",
        "sparse_x = tf.convert_to_tensor(sparse_x, dtype = tf.int32)  \n",
        "sparse_y = tf.convert_to_tensor(sparse_y, dtype = tf.int32) \n",
        "semi_hot_y = tf.convert_to_tensor(semi_hot_y, dtype = tf.int32)\n",
        "\n",
        "length = len(true_lengths_x)\n",
        "\n",
        "\n",
        "lengths_x_onehot = []\n",
        "for i in range(length):\n",
        "  lengths_x_onehot.append(tf.one_hot(true_lengths_x[i]-1, width_x))\n",
        "\n",
        "dict_size = len(list(word_dict.values()))\n",
        "\n",
        "print(\"data is ready\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHfY03CMlSvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"original dict_size:\", dict_size)\n",
        "print(\"number of phrase_pairs:\", length)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oxo5LeZqJlpy",
        "colab_type": "text"
      },
      "source": [
        "As well as TPU v1 can be executed only with static computational graph, recurrent layers have to be unrolled to train on TPU, as it needs to be unrolled all the training data must have fixed shape. \n",
        "\n",
        "In order to train seq2seq model with variable size input sequences, lenghts of train instances are encoded as one_hot for x_sequence vectors and semi_hot vectors for y_sequence (semi_hot means vector of ones of sequence lengths and zeroes for the rest).\n",
        "\n",
        "This vectors are multiplied with network output during training so non-zero gradient for each sequence will have variable lengths as training sequences and rest of the gradients will be zero.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmJzRgQEzPhN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "\n",
        "\n",
        "\n",
        "  class Seq2Seq(tf.keras.Model):\n",
        "    ''' inherited from tf.keras.model class simle seq2seq model class with one embedding layer, 2 recurrent layers and two dense layers\n",
        "    '''\n",
        "\n",
        "    def __init__(self, hidden_size, emb_size, emb_dict):\n",
        "      super(Seq2Seq, self).__init__()\n",
        "      \n",
        "      self.word_dict = emb_dict\n",
        "      self.dict_size = len(self.word_dict)\n",
        "      self.hidden_size = hidden_size\n",
        "      self.emb_size = emb_size\n",
        "\n",
        "      self.inverse_word_dict = {}\n",
        "      for key in self.word_dict.keys():\n",
        "        self.inverse_word_dict[self.word_dict[key]] = key\n",
        "\n",
        "      self.emb_layer = tf.keras.layers.Embedding(dict_size, emb_size)\n",
        "\n",
        "      self.decoder = tf.keras.layers.LSTM(units = hidden_size, kernel_regularizer=regularizers.l2(0.001), recurrent_regularizer=regularizers.l2(0.001), bias_regularizer=regularizers.l2(0.001),\n",
        "                                          dropout = 0, recurrent_dropout = 0, return_state = True, return_sequences = True, unroll = True)\n",
        "      self.encoder = tf.keras.layers.LSTM(units = hidden_size, kernel_regularizer=regularizers.l2(0.001), recurrent_regularizer=regularizers.l2(0.001), bias_regularizer=regularizers.l2(0.001),\n",
        "                                          dropout = 0, recurrent_dropout = 0, return_state = True, unroll = True)\n",
        "      \n",
        "      self.interpreter = tf.keras.Sequential([tf.keras.layers.Dense(int(hidden_size*2), activation = \"relu\"), tf.keras.layers.Dense(dict_size, activation = \"softmax\")])\n",
        "\n",
        "      hidden_representations = np.concatenate((np.zeros((1,hidden_size)), np.ones((1,hidden_size))), axis = 0)\n",
        "\n",
        "      self.emb_hidden = tf.keras.Sequential()\n",
        "      emb_hidden_layer = tf.keras.layers.Embedding(2, hidden_size, weights = [tf.constant(hidden_representations)])\n",
        "      self.emb_hidden.add(emb_hidden_layer)\n",
        "      self.emb_hidden.trainable = False\n",
        "\n",
        "      self.end_words = [\".\", \"!\",\"...\" , \"-\", \",\", \"?\"]\n",
        "    \n",
        "\n",
        "    def get_words(self, tokens):\n",
        "\n",
        "      '''fucntion that returns words from output tokens'''\n",
        "\n",
        "      words = []\n",
        "      prev_token = None\n",
        "      for token in tokens:\n",
        "        if token != prev_token:\n",
        "          words.append(self.inverse_word_dict[token])\n",
        "          prev_token = token\n",
        "          \n",
        "      return words\n",
        "  \n",
        "\n",
        "    \n",
        "    def encode_sequence(self, x_batch, one_hot, seq_len):\n",
        "\n",
        "      ''' encode sequence method, that returns hidden state for encoder for each input x_sequence'''\n",
        "\n",
        "      batch_size = np.shape(x_batch)[0]\n",
        "      hidden_total_h = tf.zeros([batch_size, self.hidden_size])\n",
        "      hidden_state_h = tf.zeros([batch_size, self.hidden_size])\n",
        "      hidden_total_c = tf.zeros([batch_size, self.hidden_size])\n",
        "      hidden_state_c = tf.zeros([batch_size, self.hidden_size])\n",
        "      hidden_state = [hidden_state_h, hidden_state_c]\n",
        "      one_hot_x = self.emb_hidden(one_hot)\n",
        "      for i in range(seq_len):\n",
        "        _, hidden_state_h, hidden_state_c = self.encoder(x_batch[:,i:i+1], hidden_state, training = True)\n",
        "        hidden_state = [hidden_state_h, hidden_state_c]\n",
        "        hidden_total_h += one_hot_x[:,i,:]*hidden_state_h\n",
        "        hidden_total_c += one_hot_x[:,i,:]*hidden_state_c\n",
        "      return [hidden_total_h, hidden_total_c]\n",
        "\n",
        "    \n",
        "    def decode_sequence(self, hidden, y_batch, semi_hot_y):\n",
        "\n",
        "      ''' method for training seq2seq with teacher \n",
        "          witch takes as argument encoded hidden state from x_input sequence\n",
        "      '''\n",
        "\n",
        "      output, _, _ = self.decoder(y_batch, hidden, training = True)\n",
        "      return self.interpreter(output)*semi_hot_y[:,:,None] + (1-semi_hot_y)[:,:,None]*tf.one_hot(0, self.dict_size, dtype = tf.float32)[None,:]\n",
        " \n",
        "\n",
        "    \n",
        "    def decode_chain_sequence(self, hidden, semi_hot_y, seq_len):\n",
        "\n",
        "      ''' method for training seq2seq without teacher \n",
        "          that takes as argument encoded hidden state and generates output as a chain sequence\n",
        "      '''\n",
        "\n",
        "      total_output = []\n",
        "      batch_size = np.shape(hidden[0])[0]\n",
        "      current_emb =  tf.zeros((batch_size, 1, self.emb_size), dtype = tf.float32)/20\n",
        "\n",
        "      for i in range(seq_len):\n",
        "        output, hidden_h, hidden_c = self.decoder(current_emb, hidden, training = True)\n",
        "        hidden = [hidden_h, hidden_c]\n",
        "        current_distribution = self.interpreter(output)*semi_hot_y[:,i:i+1,None]+(1-semi_hot_y[:,i:i+1,None])*tf.one_hot(0, self.dict_size, dtype = tf.float32)[None,:]\n",
        "        total_output.append(current_distribution)\n",
        "        current_word = tf.argmax(current_distribution, axis=-1)\n",
        "        current_emb = self.emb_layer(current_word)\n",
        "\n",
        "      return tf.concat(total_output, axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    def call(self, input_x, input_y, one_hot_x, semi_hot_y):\n",
        "\n",
        "      ''' forward method for training with techer'''\n",
        "\n",
        "      seq_len = np.shape(input_x)[1]\n",
        "      input_x = self.emb_layer(input_x)\n",
        "      input_y = self.emb_layer(input_y)\n",
        "      hidden = self.encode_sequence(input_x, one_hot_x, seq_len)\n",
        "      predictions = self.decode_sequence(hidden, input_y, semi_hot_y)\n",
        "      return predictions\n",
        "\n",
        "\n",
        "    def call_2(self, input_x, one_hot_x, semi_hot_y, seq_len_y):\n",
        "\n",
        "      ''' forward method for training without teacher'''\n",
        "\n",
        "      seq_len_x = np.shape(input_x)[1]\n",
        "      input_x = self.emb_layer(input_x)\n",
        "      hidden = self.encode_sequence(input_x, one_hot_x, seq_len_x)\n",
        "      predictions = self.decode_chain_sequence(hidden, semi_hot_y, seq_len_y)\n",
        "      return predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def decode_chain_sequence_test(self, hidden, r = 0):\n",
        "\n",
        "      ''' method applied to trained seq2seq that generate output sequence for each input phrase''' \n",
        "\n",
        "      current_emb = tf.zeros((1,1,self.emb_size), dtype = tf.float32)/20\n",
        "      total_output = []\n",
        "\n",
        "      if r > 2:\n",
        "        current_emb += tf.convert_to_tensor(np.random.rand(1, self.emb_size), dtype = tf.float32)*0.1*r/3\n",
        "\n",
        "      for _ in range(15):\n",
        "        output, hidden_h, hidden_c = self.decoder(current_emb, hidden, training = False)\n",
        "        hidden = [hidden_h, hidden_c]\n",
        "        current_distribution = self.interpreter(output)\n",
        "        current_word = tf.argmax(current_distribution, axis =-1)\n",
        "\n",
        "        if len(total_output)<1 and self.inverse_word_dict[current_word.numpy()[0,0]] in self.end_words:\n",
        "          return self.decode_chain_sequence_test(hidden, r+1)\n",
        "\n",
        "        if self.inverse_word_dict[current_word.numpy()[0,0]] == \"END\":\n",
        "          if len(total_output) < 1:\n",
        "            return self.decode_chain_sequence_test(hidden, r+1)\n",
        "          else:\n",
        "            return total_output\n",
        "\n",
        "        total_output.append(current_word.numpy()[0,0])  \n",
        "        current_emb = self.emb_layer(current_word)\n",
        "\n",
        "      return total_output  \n",
        "\n",
        "    \n",
        "    def encode_sequence_test(self, x):\n",
        "\n",
        "      ''' method applied to trained seq2seq model, returns hidden state for each input sequence'''\n",
        "\n",
        "      _ , hidden_h, hidden_c = self.encoder(x[None, :, :], training = False)\n",
        "      return [hidden_h, hidden_c]  \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    def predict(self, phrase):\n",
        "\n",
        "      ''' method applied on trained seq2seq that returns output phrase on each input phrase'''\n",
        "\n",
        "      phrase_tokens = [self.word_dict[word] for word in phrase]\n",
        "      inputs = []\n",
        "\n",
        "      for word in phrase_tokens:\n",
        "        inputs.append(self.emb_layer(word))\n",
        "\n",
        "      inputs = tf.convert_to_tensor(inputs, dtype = tf.float32) \n",
        "      hidden = self.encode_sequence_test(inputs)\n",
        "      output = self.decode_chain_sequence_test(hidden)\n",
        "      output_words = self.get_words(output)\n",
        "\n",
        "      return output_words\n",
        "\n",
        "\n",
        "      \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKtuXsxTQsdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "\t\n",
        "\t#creating embedding model with pretrained on twitter glove 50-dimensional words embeddings\n",
        "\temb_model = tf.keras.Sequential()\n",
        "\tdict_size = len(embeddings)\n",
        "\temb_size = np.shape(embeddings)[1]\n",
        "\temb_layer = tf.keras.layers.Embedding(dict_size, emb_size, weights = [tf.constant(embeddings)])\n",
        "\temb_layer.trainable = False\n",
        "\temb_model.add(emb_layer)\n",
        "\n",
        "\t#instanciating seq2seq model from seq2seq class\n",
        "\tseq2seq = Seq2Seq(HIDDEN_SIZE, EMBEDDING_SIZE, word_dict)\n",
        "\tstep1 = 0\n",
        "\tstep2 = 0\n",
        "\tlosses1 = []\n",
        "\tmean_losses1 = []\n",
        "\tlosses2 = []\n",
        "\tmean_losses2 = []\n",
        "\t\n",
        "\tLEARNING_RATE = 0.002 #0.00001\n",
        "\tloss_object = tf.keras.losses.SparseCategoricalCrossentropy(reduction=tf.keras.losses.Reduction.NONE)\n",
        " \n",
        "\n",
        "\tdef compute_loss1(labels, predictions):\n",
        "\n",
        "\t\t''' cross_entropy_loss function for distributed strategy'''\n",
        "\n",
        "\t\tper_example_loss = loss_object(labels, predictions)\n",
        "\t\treturn tf.nn.compute_average_loss(per_example_loss , global_batch_size=BATCH_SIZE)\n",
        "\t\n",
        "\n",
        "\tdef compute_loss2(labels, predictions_probabilities):\n",
        "\n",
        "\t\t''' log_probability loss for correcting output probalility distribution for distributed strategy'''\n",
        "\n",
        "\t\tprobabilities_no_grad = tf.stop_gradient(predictions_probabilities)\n",
        "\t\tpredictions_argmax = tf.argmax(probabilities_no_grad, axis = -1)\n",
        "\t\tpredictions_random = []\n",
        "\t\tfor i in range(np.shape(labels)[1]):\n",
        "\t\t\trandom_choice = tf.random.categorical(tf.math.log(probabilities_no_grad[:,i,:] + 0.0001),1)\n",
        "\t\t\tpredictions_random.append(random_choice)\n",
        "\t\tpredictions_random = tf.concat(predictions_random, axis = 1)\n",
        "\n",
        "\t\tpredictions_argmax_vector = emb_model(predictions_argmax)\n",
        "\t\tpredictions_random_vector = emb_model(predictions_random)\n",
        "\t\tlabels_vector = emb_model(labels)\n",
        "\t\n",
        "\t\trandom_similarity = tf.reduce_sum((labels_vector - predictions_random_vector)**2, axis = (1,-1))\n",
        "\t\targmax_similarity = tf.reduce_sum((labels_vector - predictions_argmax_vector)**2, axis = (1,-1))\n",
        "\t\tprobability_corrector = argmax_similarity - random_similarity\n",
        "\n",
        "\t\tlog_probabilities_corrector = -tf.math.log(predictions_probabilities+0.0001)*probability_corrector[:,None,None]\n",
        "\n",
        "\t\treturn tf.nn.compute_average_loss(log_probabilities_corrector, global_batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "\n",
        "\tdef crossentropy_training_loop( train_dist_dataset, teacher_prob, step1, step2, losses1, mean_losses1, mean_pre_loss):\n",
        "\n",
        "\t\t''' training function for crossentropy training'''\n",
        "\n",
        "\t\tfor x in train_dist_dataset:\n",
        "\n",
        "\t\t\tif np.random.rand() < teacher_prob:\n",
        "\t\t\t\tstep1+=1\n",
        "\t\t\t\tloss = distributed_train_step1(x)\n",
        "\t\t\t\tlosses1.append(loss)\n",
        "\t\t\t\tif step1 > 50:\n",
        "\t\t\t\t\tmean_losses1.append(np.mean(losses1[-50:]))\n",
        "\t\t\telse:\n",
        "\t\t\t\tstep2+=1\n",
        "\t\t\t\tloss = distributed_train_step2(x)\n",
        "\t\t\t\tlosses2.append(loss)\n",
        "\t\t\t\tif step2 > 50:\n",
        "\t\t\t\t\tmean_losses2.append(np.mean(losses2[-50:]))\n",
        "\t\t\n",
        "\t\t\tmean_pre_loss.append(loss)\n",
        "\t \n",
        "\n",
        "\tdef distribution_correction_loop( train_dist_dataset, teacher_prob, step1, step2, losses1, mean_losses1, mean_pre_loss):\n",
        "\n",
        "\t\t''' training function for output probability distribution correction'''\n",
        "\n",
        "\t\tfor x in train_dist_dataset:\n",
        "\n",
        "\t\t\tif np.random.rand() < teacher_prob:\n",
        "\t\t\t\tstep1+=1\n",
        "\t\t\t\tloss = distributed_train_step4(x)\n",
        "\t\t\t\tlosses1.append(loss)\n",
        "\t\t\t\tif step1 > 50:\n",
        "\t\t\t\t\tmean_losses1.append(np.mean(losses1[-50:]))\n",
        "\t\t\telse:\n",
        "\t\t\t\tstep2+=1\n",
        "\t\t\t\tloss = distributed_train_step3(x)\n",
        "\t\t\t\tlosses2.append(loss)\n",
        "\t\t\t\tif step2 > 50:\n",
        "\t\t\t\t\tmean_losses2.append(np.mean(losses2[-50:]))\n",
        "\t\t\n",
        "\t\t\tmean_pre_loss.append(loss)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\tdef distributed_train_step1(dataset_inputs):\n",
        "\n",
        "\t\t''' function that applies distriuted TPU strategy to training_step1 fucntion'''\n",
        "\n",
        "\t\tper_replica_losses = tpu_strategy.experimental_run_v2(train_step1,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\targs=(dataset_inputs,))\n",
        "\t\treturn tpu_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\taxis=None)\n",
        "\n",
        "\tdef distributed_train_step2(dataset_inputs):\n",
        "\n",
        "\t\t''' function that applies distriuted TPU strategy to training_step2 fucntion'''\n",
        "\n",
        "\t\tper_replica_losses = tpu_strategy.experimental_run_v2(train_step2,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\targs=(dataset_inputs,))\n",
        "\t\treturn tpu_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\taxis=None)\t\n",
        "\t\n",
        "\tdef distributed_train_step3(dataset_inputs):\n",
        "\n",
        "\t\t''' function that applies distriuted TPU strategy to training_step3 fucntion'''\n",
        "\n",
        "\t\tper_replica_losses = tpu_strategy.experimental_run_v2(train_step3,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\targs=(dataset_inputs,))\n",
        "\t\treturn tpu_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\taxis=None)\n",
        "\n",
        "\tdef distributed_train_step4(dataset_inputs):\n",
        "\n",
        "\t\t''' function that applies distriuted TPU strategy to training_step1 fucntion'''\n",
        "\n",
        "\t\tper_replica_losses = tpu_strategy.experimental_run_v2(train_step4,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\targs=(dataset_inputs,))\n",
        "\t\treturn tpu_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\taxis=None)\n",
        "\n",
        "\n",
        "\tfor _ in range(7):\n",
        "\n",
        "\t\t''' main training_loop'''\n",
        "\n",
        "\t\toptimizer1 = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1 = 0.9)\n",
        "\t\toptimizer2 = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1 = 0.9)\n",
        "\t\toptimizer3 = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE*0.005, beta_1 = 0.9)\n",
        "\t\toptimizer4 = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE*0.005, beta_1 = 0.9)\n",
        "\t\tLEARNING_RATE /= 2\t\n",
        "\t\t\n",
        "\n",
        "\t\t@tf.function\n",
        "\t\tdef train_step1(inputs):\n",
        "\n",
        "\t\t\t''' training_step1 fuction for crossentropy teacher training method \n",
        "\t\t\t\t\twith @tf.function decorator that assures constriction of static computation graph for training\n",
        "\t\t\t'''\n",
        "\n",
        "\t\t\twith tf.GradientTape() as tape:\n",
        "\t\t\t\tinput_x = inputs[0]\n",
        "\t\t\t\tinput_y = inputs[1]\n",
        "\t\t\t\tone_hot_x = inputs[2]\n",
        "\t\t\t\tsemi_hot_y = tf.cast(inputs[3], tf.float32)\n",
        "\t\t\t\tpredictions = seq2seq(input_x, input_y[:,:-1], one_hot_x, semi_hot_y[:,1:])\n",
        "\t\t\t\tloss = compute_loss1(input_y[:,1:], predictions)\n",
        "\t\t\t\t#print_loss = compute_loss1(input_y[:,1:], predictions)\n",
        "\t\t\tgradients = tape.gradient(loss, seq2seq.trainable_variables)\n",
        "\t\t\toptimizer1.apply_gradients(zip(gradients, seq2seq.trainable_variables))\n",
        "\t\t\treturn loss\n",
        "\t\t\n",
        "\t\t@tf.function\n",
        "\t\tdef train_step2(inputs):\n",
        "\n",
        "\t\t\t''' training_step1 fuction for crossentropy sequence training method \n",
        "\t\t\t\t\twith @tf.function decorator that assures constriction of static computation graph for training\n",
        "\t\t\t'''\n",
        "\n",
        "\t\t\twith tf.GradientTape() as tape:\n",
        "\t\t\t\tinput_x = inputs[0]\n",
        "\t\t\t\tinput_y = inputs[1]\n",
        "\t\t\t\tone_hot_x = inputs[2]\n",
        "\t\t\t\tsemi_hot_y = tf.cast(inputs[3], tf.float32)\n",
        "\t\t\t\tpredictions = seq2seq.call_2(input_x, one_hot_x, semi_hot_y[:,1:], np.shape(input_y)[1]-1)\n",
        "\t\t\t\tloss = compute_loss1(input_y[:,1:], predictions)\n",
        "\t\t\tgradients = tape.gradient(loss, seq2seq.trainable_variables)\n",
        "\t\t\toptimizer2.apply_gradients(zip(gradients, seq2seq.trainable_variables))\n",
        "\t\t\treturn loss\n",
        "\t\t\n",
        "\t\t@tf.function\n",
        "\t\tdef train_step3(inputs):\n",
        "\n",
        "\t\t\t''' training_step1 fuction for output probability distribution correction training method \n",
        "\t\t\t\t\twith @tf.function decorator that assures constriction of static computation graph for training\n",
        "\t\t\t'''\n",
        "\n",
        "\t\t\twith tf.GradientTape() as tape:\n",
        "\t\t\t\tinput_x = inputs[0]\n",
        "\t\t\t\tinput_y = inputs[1]\n",
        "\t\t\t\tone_hot_x = inputs[2]\n",
        "\t\t\t\tsemi_hot_y = tf.cast(inputs[3], tf.float32)\n",
        "\t\t\t\tpredictions = seq2seq.call_2(input_x, one_hot_x, semi_hot_y[:,1:], np.shape(input_y)[1]-1)\n",
        "\t\t\t\tloss = compute_loss2(input_y[:,1:], predictions)\n",
        "\t\t\t\tprinted_loss = compute_loss1(input_y[:,1:], predictions)\n",
        "\t\t\tgradients = tape.gradient(loss, seq2seq.trainable_variables)\n",
        "\t\t\toptimizer3.apply_gradients(zip(gradients, seq2seq.trainable_variables))\n",
        "\t\t\treturn printed_loss\n",
        "\t\t\n",
        "\n",
        "\n",
        "\t\t@tf.function\n",
        "\t\tdef train_step4(inputs):\n",
        "\n",
        "\t\t\t''' training_step1 fuction for crossentropy teacher training method \n",
        "\t\t\t\t\twith @tf.function decorator that assures constriction of static computation graph for training\n",
        "\t\t\t'''\n",
        "\n",
        "\t\t\twith tf.GradientTape() as tape:\n",
        "\t\t\t\tinput_x = inputs[0]\n",
        "\t\t\t\tinput_y = inputs[1]\n",
        "\t\t\t\tone_hot_x = inputs[2]\n",
        "\t\t\t\tsemi_hot_y = tf.cast(inputs[3], tf.float32)\n",
        "\t\t\t\tpredictions = seq2seq(input_x, input_y[:,:-1], one_hot_x, semi_hot_y[:,1:])\n",
        "\t\t\t\tloss = compute_loss2(input_y[:,1:], predictions)\n",
        "\t\t\t\tprint_loss = compute_loss1(input_y[:,1:], predictions)\n",
        "\t\t\tgradients = tape.gradient(loss, seq2seq.trainable_variables)\n",
        "\t\t\toptimizer4.apply_gradients(zip(gradients, seq2seq.trainable_variables))\n",
        "\t\t\treturn print_loss\n",
        "\n",
        "\t\tif _ < 6:\n",
        "\t\t\tprint(\"crossentropy training...\")\n",
        "\n",
        "\t\t\tteacher_prob = 1\n",
        "\t\t\tn_epochs = int(80)\n",
        "\n",
        "\t\t\tif _ > 1:\n",
        "\t\t\t\tteacher_prob = 0.5\n",
        "\t\t\t\tn_epochs  = 24 - _*2\n",
        "\n",
        "\t\t\tfor epoch in range(n_epochs):\n",
        "\t\t\t\tepoch_start = time.localtime(time.time())\n",
        "\t\t\t\t\n",
        "\t\t\t\tmean_pre_loss = []\n",
        "\t\t\t\tdataset = tf.data.Dataset.from_tensor_slices((sparse_x, sparse_y, lengths_x_onehot, semi_hot_y)).shuffle(150000).batch(BATCH_SIZE, drop_remainder = True) \n",
        "\t\t\t\ttrain_dist_dataset = tpu_strategy.experimental_distribute_dataset(dataset)\n",
        "\t\t\t\n",
        "\t\t\t\tif (epoch > 35 or _ > 0) and (epoch+1) % 20 == 0:\n",
        "\t\t\t\t\tteacher_prob = 1 - teacher_prob\n",
        "\t\n",
        "\n",
        "\t\t\t\tcrossentropy_training_loop(train_dist_dataset, teacher_prob, step1, step2, losses1, mean_losses1, mean_pre_loss)\n",
        "\t\t\t\n",
        "\t\t\t\tepoch_end = time.localtime(time.time())\n",
        "\t\t\t\tstart_in_sec = epoch_start[3]*3600 + epoch_start[4]*60 + epoch_start[5]\n",
        "\t\t\t\tend_in_sec = epoch_end[3]*3600 + epoch_end[4]*60 + epoch_end[5]\n",
        "\t\t\t\tepoch_time = end_in_sec - start_in_sec\n",
        "\n",
        "\t\t\t\tprint(\"Cycle: \", _ , \", Epoch: \", epoch,\", And current loss is:\", np.mean(mean_pre_loss), \" while teacher_prob is: \", teacher_prob, \", Epoch_time: \", epoch_time)\t\n",
        "\t\t\t\n",
        "\t\telse:\n",
        "\n",
        "\t\t\tprint(\"distribution correction...\")\n",
        "\n",
        "\t\t\tn_epochs = 300\n",
        "\t\t\t\n",
        "\t\t\tteacher_prob = 0.5\n",
        "\n",
        "\t\t\tfor epoch in range(n_epochs):\n",
        "\t\t\t\tepoch_start = time.localtime(time.time())\n",
        "\n",
        "\t\t\t\t\n",
        "\t\t\t\tmean_pre_loss = []\n",
        "\t\t\t\tdataset = tf.data.Dataset.from_tensor_slices((sparse_x, sparse_y, lengths_x_onehot, semi_hot_y)).shuffle(150000).batch(BATCH_SIZE, drop_remainder = True) \n",
        "\t\t\t\ttrain_dist_dataset = tpu_strategy.experimental_distribute_dataset(dataset)\n",
        "\n",
        "\t\t\t\tdistribution_correction_loop( train_dist_dataset, teacher_prob, step1, step2, losses1, mean_losses1, mean_pre_loss)\n",
        "\t\t\t\n",
        "\t\t\t\tepoch_end = time.localtime(time.time())\n",
        "\t\t\t\tstart_in_sec = epoch_start[3]*3600 + epoch_start[4]*60 + epoch_start[5]\n",
        "\t\t\t\tend_in_sec = epoch_end[3]*3600 + epoch_end[4]*60 + epoch_end[5]\n",
        "\t\t\t\tepoch_time = end_in_sec - start_in_sec\n",
        "\n",
        "\t\t\t\tprint(\"Cycle: \", _ , \", Epoch: \", epoch,\", And current loss is:\", np.mean(mean_pre_loss), \" while teacher_prob is: \", teacher_prob, \", Epoch_time: \", epoch_time)\n",
        "\t\t\t\t\n",
        "\t\t\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "czUlB75mqA40",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "print(\"seq2seq is trained\")\n",
        "plt.plot(mean_losses1)\n",
        "plt.show()\n",
        "plt.close()\n",
        "plt.plot(mean_losses2)\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "\n",
        "my_questions = [\"are you a human ?\", \n",
        "                \"what is important to you ?\",\"hello ! how are you ?\",\"hello !\",\"what is your business here ?\",\n",
        "                \"tell me something about humans\",\n",
        "                \"what i should do now ?\",\n",
        "                \"are you good guy ?\", \"who are you ?\",\n",
        "                \"do you aim to harm humanity ?\",\"what is on your mind ?\",\n",
        "                \"can you be my friend ?\",\"are you a machine ?\",\n",
        "                \"how can i help you ?\",\"what can you say about yourself ?\",\n",
        "                \"will artificial intelligence rule the world ?\",\n",
        "                \"are you smart enough ?\",\n",
        "                \"it is bad weather today\",\n",
        "                \"how can you explain your existence ?\"]\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "def talk_to_me(test_phrases):\n",
        "  converted_test_phrases = []\n",
        "  converted_test_answers = []\n",
        "  test_answers = []\n",
        "\n",
        "  with tf.device(\"cpu:0\"):\n",
        "\n",
        "    for phrase in test_phrases:\n",
        "      phrase_words = phrase.split(\" \")\n",
        "      skip = False\n",
        "      for word in phrase_words:\n",
        "        if not word in word_dict.keys():\n",
        "          test_answers.append([\"sorry\" , \",\", \"i\", \"dont\", \"know\", \"what\", \"'\" + word + \"'\", \"means.\"])\n",
        "          skip = True\n",
        "          break\n",
        "          \n",
        "\n",
        "      if not skip:\n",
        "        predicted = seq2seq.predict(phrase.strip(\" \").split(\" \"))\n",
        "        test_answers.append(predicted)\n",
        "\n",
        "    \n",
        "  for i in range(len(test_phrases)):\n",
        "    phrase = test_phrases[i].strip(\" \").split(\" \")\n",
        "\n",
        "    answer = test_answers[i]\n",
        "    converted_answer = \"\"\n",
        "    for word in answer:\n",
        "      converted_answer += word\n",
        "      converted_answer += \" \"\n",
        "\n",
        "    converted_test_answers.append(converted_answer)  \n",
        "\n",
        "\n",
        "  for i in range(len(test_phrases)):\n",
        "    print(test_phrases[i])\n",
        "    print(converted_test_answers[i])\n",
        "    print(\"\\n\")  \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vu2K_EI2J0PD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "talk_to_me(my_questions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSSjrzxgxxjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "talk_to_me([\"are you a artificial mind ?\"])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}