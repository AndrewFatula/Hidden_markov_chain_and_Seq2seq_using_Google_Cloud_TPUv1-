{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYLsOQYI7oSuXSf8EOgQ8c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndrewFatula/Seq2seq_model_using_Google_Cloud_TPU/blob/master/Seq2seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlkoGCLQ0Ahe",
        "colab_type": "code",
        "outputId": "66fd22b5-e943-408a-a58e-106d0aabaefc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import numpy as np\n",
        "import string\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "import tensorflow as tf\n",
        "from collections import Counter\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.translate import bleu_score\n",
        "from keras import regularizers\n",
        "from copy import deepcopy as dc\n",
        "import tensorboardcolab as tb\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJ1_citv8gLk",
        "colab_type": "text"
      },
      "source": [
        "Importing all needed packages."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "On17B68F0E17",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install --upgrade auth\n",
        "!pip uninstall grpcio\n",
        "!pip uninstall tensorflow\n",
        "!pip install grpcio==1.24.3\n",
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGBhGW7W8ciG",
        "colab_type": "text"
      },
      "source": [
        "Default version of tensorflow in colab is 1.15, so in order to gain all the benefits of tensorflow 2.0, current version of tensorflow needs to be uninstalled and then we can install the 2.0 version, but in order to use google cloud TPU v1 with tensorflow 2.0 before installing 2.0 version of tensorflow packages like grpcio and auth should be reinstalled the same way to versions specified above in code.\n",
        "\n",
        "When given packages are reinstalled runtime have to be reset in order to activates updates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DCCb17s0GH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq0z-8NO8plk",
        "colab_type": "text"
      },
      "source": [
        "Importing all needed tools and authentificating in google account in order to import data in colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JHVzWcF0Hr1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':'1Z7odQLtZ7RmWERaRDJHDvbX_3VJ2ptvu'}) # all phrases in movies\n",
        "downloaded.GetContentFile('movie_lines.txt') \n",
        "downloaded = drive.CreateFile({'id':'1TdbFyBvMGV_N8iszqTIAb7V1Vx_eSdMr'}) # all conversations in movies\n",
        "downloaded.GetContentFile('movie_conversations.txt') \n",
        "downloaded = drive.CreateFile({'id':'1YTQeB3x_HTeEA5sjrleJD3notEKaPBnM'}) # all titles of movies\n",
        "downloaded.GetContentFile('movie_titles_metadata.txt')\n",
        "downloaded = drive.CreateFile({'id':'16Lxkrsd8HV9j9IrueYXQwY0TMbSttx5P'}) # GLOVE 50-demensional pretrained vector representations for 400000 English words trained on data from Twitter \n",
        "downloaded.GetContentFile('glove.6B.50d.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NW-JqRfE8oxU",
        "colab_type": "text"
      },
      "source": [
        "Loading all needed datafiles for constucting training dataset including GLOVE 50-demensional pretrained vector representations for 400000 English words trained on data from Twitter - which is needed in training seq2seq model to check phrases similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUZknwlN0JA_",
        "colab_type": "code",
        "outputId": "161f9f2d-184e-4728-a72a-ef77e03a87f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 763
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "if 'COLAB_TPU_ADDR' not in os.environ:\n",
        "  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')\n",
        "else:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  print ('TPU address iis', tpu_address)\n",
        "\n",
        "\n",
        "cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(\n",
        "    tpu=tpu_address)\n",
        "tf.config.experimental_connect_to_cluster(cluster_resolver)\n",
        "tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)\n",
        "\n",
        "print ('Number of devices: {}'.format(tpu_strategy.num_replicas_in_sync))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TPU address iis grpc://10.75.12.114:8470\n",
            "INFO:tensorflow:Initializing the TPU system: 10.75.12.114:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: 10.75.12.114:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Number of devices: 8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGfNg7sR8z_V",
        "colab_type": "text"
      },
      "source": [
        "Setting up TPU v1 hardware acceleration system. \n",
        "Definig TPU cluster_resolver and tpu_distributed_strategy for training seq2seq."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwM0dzb40Kc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "SEPARATOR = \"+++$+++\"\n",
        "MAX_TOKENS_X = 20\n",
        "MAX_TOKENS_Y = 15\n",
        "MIN_TOKEN_FREQ = 3\n",
        "\n",
        "VEC_SIZE = 50\n",
        "\n",
        "EMB_FILE = 'glove.6B.50d.txt'\n",
        "GENRES = \"comedy,romance,sci-fi\"\n",
        "WITH_EXCEPTION = False\n",
        "\n",
        "\n",
        "def tokenize(str_):\n",
        "\treturn TweetTokenizer(preserve_case=False).tokenize(str_)\n",
        "\n",
        "\n",
        "def remove_all(str, substrings):\n",
        "\tindex = 0\n",
        "\tfor substr in substrings:\n",
        "\t\tlength = len(substr)\n",
        "\t\twhile str.find(substr) != -1:\n",
        "\t\t\tindex = str.find(str)\n",
        "\t\t\tstr = str[0 : index] + str[index + length:]\n",
        "\treturn str\n",
        "\n",
        "\n",
        "\n",
        "def load_movies(genres, exception = False):\n",
        "\n",
        "\t'''This function loads all the movie titles of specicfied genres from <movie_titles_metadata.txt> file and returns list of movie titles'''\n",
        "\n",
        "\n",
        "\tmovies = []\n",
        "\twith open(\"movie_titles_metadata.txt\", 'rb') as gf:\n",
        "\n",
        "\t\tfor line in gf:\n",
        "\t\t\tline = str(line, encoding='utf-8', errors='ignore')\n",
        "\t\t\tarr_line = list(map( lambda x: x.strip(), line.split(SEPARATOR) ))\n",
        "\t\t\tline_genres = list(map( lambda x: x.strip(\" '\"), arr_line[-1].strip(\"[]\").split(\",\") ))\n",
        "\t\t\tappend = True\n",
        "   \n",
        "\t\t\tfor genre in genres:\n",
        "\t\t\t\tif not exception:\n",
        "\t\t\t\t\tappend = False\n",
        "\t\t\t\t\tif genre in line_genres:\n",
        "\t\t\t\t\t\tappend = True\n",
        "\t\t\t\t\t\tbreak\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tif genre in line_genres:\n",
        "\t\t\t\t\t\tappend = False\n",
        "\t\t\t\t\t\tbreak\n",
        "\n",
        "\t\t\tif append:\n",
        "\t\t\t\tmovies.append(arr_line[0])\n",
        "\t\t\t\t\n",
        "\n",
        "\treturn movies\t\t\t\t\n",
        "\n",
        "\n",
        "def read_phrases(movies = [], genres = []):\n",
        "\n",
        "\t''' This function loads all phrasses from <movie_lines.txt> file which are said in given movie list as an argument of this function\n",
        "\t\t\tIf function is called with no argument than it reads phrasses from all the movies available in dataset  '''\n",
        "\n",
        "\tphrases = {}\n",
        "\n",
        "\twith open('movie_lines.txt', 'rb') as lf:\n",
        "\n",
        "\t\tfor line in lf:\n",
        "\t\t\tline = str(line, encoding='utf-8', errors='ignore').replace(\"<u>\",\"\").replace(\"</u>\",\"\")\n",
        "\n",
        "\t\t\tarr_line = list(map( lambda x: x.strip(), line.split(SEPARATOR) ))\n",
        "\t\t\t#phrases are loaded in dictionary, the key is index of a conversation\n",
        "\t\t\t\n",
        "\t\t\tif not movies:\n",
        "\t\t\t\tphrases[arr_line[0]] = tokenize(remove_all(arr_line[-1],[\"<b>\",\"</b>\",\"<u>\",\"</u>\",\"<i>\",\"</i>\",\"<u>\", \"</u>\"]))\n",
        "\t\t\telif arr_line[2] in movies: \n",
        "\t\t\t\tphrases[arr_line[0]] = tokenize(remove_all(arr_line[-1],[\"<b>\",\"</b>\",\"<u>\",\"</u>\",\"<i>\",\"</i>\",\"<u>\", \"</u>\"]))\n",
        "\t\n",
        "\t\n",
        "\treturn phrases\t\n",
        "\t\n",
        "\n",
        "\n",
        "def read_dialogues(phrases, movies):\n",
        "\n",
        "\t''' this function constructs dialogs from phrases in the movies,\n",
        "\t\t\tall the needed information for dialogs construction is readed from file <movie_conversations.txt>'''\n",
        "\n",
        "\tdialogues = []\n",
        "\twith open(\"movie_conversations.txt\", 'rb') as df:\n",
        "\n",
        "\t\tfor line in df:\t\n",
        "\t\t\tline = str(line, encoding='utf-8', errors='ignore')\n",
        "\t\t\tarr_line = list(map( lambda x: x.strip(), line.split(SEPARATOR) ))\n",
        "\t\t\tdialog = list(map( lambda x: x.strip(\"' \"), arr_line[-1].strip(\"[]\").split(\",\") ))\n",
        "\n",
        "\t\t\tif not movies:\n",
        "\t\t\t\tdialogues.append([phrases[phrase] for phrase in dialog])\n",
        "\t\t\telif arr_line[2] in movies:\t\n",
        "\t\t\t\tdialogues.append([phrases[phrase] for phrase in dialog])\n",
        "\n",
        "\treturn dialogues\t\t\t\t\n",
        "\n",
        "\n",
        "def get_phrase_pairs(genres = None, max_tokins = MAX_TOKENS_X, n_pairs = None, exception = False):\n",
        "\n",
        "\t''' This function constructs phrase_pairs dataset where each instance of dataset is phrase and response to that phrase '''\n",
        "\n",
        "\t#when genres in not specified it read phrses from all the movies is available\n",
        "\tif genres == None:\n",
        "\t\tall_phrases = read_phrases()\n",
        "\t\tmovies = []\n",
        "\telse:\n",
        "\t\tmovies = load_movies(genres, exception = exception)\n",
        "\t\tall_phrases = read_phrases(movies, genres = genres)\t\n",
        "\t\n",
        "\t#before constructing phrase pairs dataset we need to get conversations dataset from all readed phrases\n",
        "\tconversations = read_dialogues(all_phrases, movies)\n",
        "\tphrase_pairs = []\n",
        "\n",
        "\tfor conv in conversations:\n",
        "\t\tprev_phrase = None\n",
        "\n",
        "\t\tfor phrase in conv:\n",
        "\t\t\tif prev_phrase is not None and (max_tokins == None or (len(phrase) <= max_tokins and len(prev_phrase) <= max_tokins)):\n",
        "\t\t\t\tphrase_pairs.append((prev_phrase, phrase))\n",
        "\t\t\tprev_phrase = phrase\n",
        "\n",
        "\tif not len(genres) == 0:\n",
        "\t\tif not exception:\n",
        "\t\t\tprint(\"Total number of loaded phrases in \" + \",\".join(genres) + \" movies is :\", len(phrase_pairs))\n",
        "\t\telse:\n",
        "\t\t\tprint(\"Total number of loaded phrases not in \" + \",\".join(genres) + \" movies is :\", len(phrase_pairs))\t\t\n",
        "\telse:\n",
        "\t\tprint(\"Total number of loaded phrases of all movies : \" ,len(phrase_pairs))\n",
        "\n",
        "\tif (n_pairs == None) or (n_pairs >= len(phrase_pairs)) :\n",
        "\t\treturn phrase_pairs, conversations\n",
        "\telse:\n",
        "\t\tnp.random.shuffle(phrase_pairs)\n",
        "\t\treturn phrase_pairs[0:n_pairs], conversations\n",
        "\n",
        "\t\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1jhJxpq9Izw",
        "colab_type": "text"
      },
      "source": [
        "Fuctions above are used to construct phrase_pairs dataset from movies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAj1fKDE0MGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def read_embeddings(filepath):\n",
        "\n",
        "\t'''this function reads pretrained on twitter data word2vec embeddings from embedings file, \n",
        "\t\t it returns list of vector representations of each word in embeddings file and token words dictionary'''\n",
        "\n",
        "\twords = []\n",
        "\twords.append(\"EMPTY\")\n",
        "\twords.append(\"BEGIN\")\n",
        "\twords.append(\"END\")\n",
        "\twords_dict = {}\n",
        "\trepresentations = []\n",
        "\trepresentations.append([0] * VEC_SIZE)\n",
        "\trepresentations.append([1] * VEC_SIZE)\n",
        "\trepresentations.append([2] * VEC_SIZE)\n",
        "\n",
        "\twith open(filepath) as fp:\n",
        "\t\tline = fp.readline()\n",
        "\t\twhile line:\n",
        "\t\t\tline_content = line.split(\" \")\n",
        "\t\t\twords.append(line_content[0])\n",
        "\t\t\trepresentations.append(list(map(lambda x: float(x), line_content[1:])))\n",
        "\t\t\tline = fp.readline()\n",
        "\t \n",
        "\tfor i in range( len(words) - 1 ):\n",
        "\t\twords_dict[words[i]] = i\n",
        "\n",
        "\treturn np.array(representations[:-1]), words_dict\n",
        "\n",
        "\n",
        "\n",
        "def word_corrector(word):\n",
        "\n",
        "\t''' this function is used to convert commonly used english words shortcuts to corresponding them full forms '''\n",
        "\n",
        "\tdots = ['. ..', '. . .', '..', '. ...','...  ...', '.  ...', '. .']\n",
        "\tcomonly_used = [\"that\", \"what\", \"there\", \"who\", \"where\", \"how\"]\n",
        "\tpronouns = [\"it\", \"she\", \"he\"]\n",
        "\n",
        "\tif word[-3:] == \"'ll\":\n",
        "\t\tdecoded = [ word[:-3] , \"will\" ]\n",
        "\t\treturn decoded\n",
        "\n",
        "\telif word[-2:] == \"'d\":\n",
        "\t\tdecoded = [ word[:-2] , \"would\" ]\n",
        "\t\treturn decoded \n",
        "\n",
        "\telif word[-2:] == \"'s\" or word[-2:] == \"s'\": \n",
        "\t\tif not word[:-2] in comonly_used+pronouns:\n",
        "\t\t\tdecoded =  [ \"someone\" , \"'\" , \"s\" ]\n",
        "\t\telse:\n",
        "\t\t\tdecoded = [word]\n",
        "\t\treturn decoded\n",
        "\n",
        "\telif word[-2:] == \"'t\":\n",
        "\t\t\n",
        "\t\tif word == \"can't\":\n",
        "\t\t\tdecoded = [ \"can\" , \"not\" ]\n",
        "\t\telif word == \"won't\":\n",
        "\t\t\tdecoded = [ \"will\" , \"not\" ]\n",
        "\t\telse:\n",
        "\t\t\tdecoded = [ word[:-3] , \"not\" ]\t\n",
        "\t\treturn decoded   \n",
        "\n",
        "\telif word[-3:] == \"'re\":\n",
        "\t\tdecoded = [ word[:-3] , \"are\" ]\n",
        "\t\treturn decoded \n",
        "\n",
        "\telif word == \"i'm\":\n",
        "\t\tdecoded = [ \"i\", \"am\" ]\n",
        "\t\treturn decoded \n",
        "\n",
        "\telif word[-3:] == \"'ve\":\n",
        "\t\tdecoded = [ word[:-3] , \"have\" ]\n",
        "\t\treturn decoded \n",
        "\n",
        "\telif word[:2] == \"y'\":\n",
        "\t\tdecoded = [ \"you\" , word[2:] ]\n",
        "\t\treturn decoded\n",
        "\n",
        "\telif word[:2] == \"dont\":\n",
        "\t\tdecoded = [ \"do\" , \"not\" ]\n",
        "\t\treturn decoded\n",
        "\telse:\n",
        "\t\tif word in dots:\n",
        "\t\t\tword = \"...\"\n",
        "\t\telif word == \"u\":\n",
        "\t\t\tword = \"you\"\n",
        "\t\telif word == \"ur\":\n",
        "\t\t\tword = \"your\"\t\n",
        "\n",
        "\t\treturn [word]\n",
        "\n",
        "####### \n",
        "def get_word_dict2(phrase_pairs, emb_dict):\n",
        "\n",
        "\t''' this function constructs token dictionary with words which is available in embeddings dictionary retrieved from GLOVE 50-dimensional wordvectors representations,\n",
        "\t\t\tand is present in phrase_pairs dataset with frequency >= than specified MIN_TOKEN_FREQ '''\n",
        "\n",
        "\tfreq_count = Counter()\n",
        "\tsizes_x = []\n",
        "\tsizes_y = []\n",
        "\n",
        "\tfor pair in phrase_pairs:\n",
        "\t\tif len(pair[0]) < MAX_TOKENS_X + 1 and len(pair[1]) < MAX_TOKENS_Y + 1:\n",
        "\t\t\tfreq_count.update(pair[0])\n",
        "\t\t\tfreq_count.update(pair[1])\n",
        "\n",
        "\tword_set = list(map(lambda x: '+' + x[0] if x[1] >= MIN_TOKEN_FREQ else '-' + x[0], freq_count.items() ))\n",
        "\tword_dict = {\"EMPTY\":0, \"BEGIN\":1, \"END\" : 2}\n",
        "\ti = 3\n",
        "\tn=0\n",
        "\tfor word in word_set:\n",
        "\t\tif word[0] == \"+\":\n",
        "\t\t\tn+=1\n",
        "\t\t\tcorrect_word = word_corrector(word[1:])\n",
        "   \n",
        "\t\t\tfor corrected in correct_word:\n",
        "\t\t\t\tif not corrected in word_dict.keys() :\n",
        "\t\t\t\t\tword_dict[corrected] = i\n",
        "\t\t\t\t\ti+=1\n",
        "\treturn word_dict\n",
        "\n",
        "\n",
        "\n",
        "def convert_phrases2(phrase_pairs, word_dict):\n",
        "\n",
        "\t''' this function converts all the words in phrases dataset to tokens based on token dictionary retrieved from embeddings file and phrase_pairs dataset,\n",
        "\t\t\tphrases with words which frequencies are < MIN_TOKEN_FREQ or with words which are unavailable in words embeddings file are ignored,\n",
        "\t\t\talso it converts common words shortcuts in phrases to corresponding them full forms, \n",
        "\t\t\tit returns converted to tokens phrase_pairs separately and lengths for each phrase in pair '''\n",
        "\n",
        "\tsizes_x = []\n",
        "\tsizes_y = []\n",
        "\tconverted_x = []\n",
        "\tconverted_y = []\n",
        "\tconverted_x_reverse = []\n",
        "\n",
        "\tfor pair in phrase_pairs:\n",
        "\t\tif len(pair[0]) < MAX_TOKENS_X + 1 and len(pair[1]) < MAX_TOKENS_Y + 1:\n",
        "\t\t\tphrase1 = []\n",
        "\t\t\tphrase2 = []\n",
        "\t\t\tphrase1_reverse = []\n",
        "\t\t\tfor word in pair[0]:\n",
        "\t\t\t\tcorrect_word = word_corrector(word)\n",
        "\n",
        "\t\t\t\tfor correct in correct_word:\n",
        "\t\t\t\t\tif correct in word_dict.keys():\n",
        "\t\t\t\t\t\tphrase1.append(word_dict[correct])\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tphrase1.append(word_dict[\"EMPTY\"])\n",
        "\n",
        "\t\t\tfor word in pair[1]:\n",
        "\t\t\t\tcorrect_word = word_corrector(word)\n",
        "\n",
        "\t\t\t\tfor correct in correct_word:\n",
        "\t\t\t\t\tif correct in word_dict.keys():\n",
        "\t\t\t\t\t\tphrase2.append(word_dict[correct])\n",
        "\t\t\t\t\telse:\n",
        "\t\t\t\t\t\tphrase2.append(word_dict[\"EMPTY\"])\n",
        "\n",
        "\t\t\tphrase1 = [1] + phrase1 + [2]\n",
        "\t\t\tphrase2 = [1] + phrase2 + [2]\n",
        "\t\t\n",
        "\t\t\tif not word_dict[\"EMPTY\"] in phrase1[1:] and not word_dict[\"EMPTY\"] in phrase2[1:] and not (len(phrase1) > MAX_TOKENS_X + 8 or len(phrase2) > MAX_TOKENS_Y + 8):\n",
        "\t\t\t\tl1 = len(phrase1)\n",
        "\t\t\t\tfor token_idx in range(l1):\n",
        "\t\t\t\t\tphrase1_reverse.append(phrase1[l1-token_idx-1])\n",
        "\t \n",
        "\t\t\t\tconverted_x.append(phrase1)\n",
        "\t\t\t\tconverted_y.append(phrase2)\n",
        "\t\t\t\tconverted_x_reverse.append(phrase1_reverse)\n",
        "\n",
        "\t\t\t\tsizes_x.append(len(phrase1))\n",
        "\t\t\t\tsizes_y.append(len(phrase2))\n",
        "\t\n",
        "\tlength = len(sizes_x)\n",
        "\tsizes_x = np.array(sizes_x)\n",
        "\tsizes_y = np.array(sizes_y)\n",
        "\n",
        "\treturn converted_x, converted_y, converted_x_reverse, sizes_x.astype(np.int32), sizes_y.astype(np.int32)\n",
        "\n",
        "def make_addition(all_phrases):\n",
        "\tphrases = []\n",
        "\n",
        "\tphrases.append(([\"hi\",\"!\"],[\"hello\"]))\n",
        "\tphrases.append(([\"hello\",\"!\"],[\"hi\", \"!\"]))\n",
        "\tphrases.append(([\"hello\",\"!\"],[\"hi\", \".\"]))\n",
        "\tphrases.append(([\"hello\",\".\"],[\"hi\", \"!\"]))\n",
        "\tphrases.append(([\"hello\"],[\"hi\", \".\"]))\n",
        "\tphrases.append(([\"hello\"],[\"hi\", \"!\"]))\n",
        "\tphrases.append(([\"hi\"],[\"hello\", \".\"]))\n",
        "\tphrases = phrases * 4\n",
        "\tphrases.append(([\"hi\",\".\"],[\"hello\", \",\",\"nice\", \"to\", \"meet\",\"you\"]))\n",
        "\tphrases.append(([\"hi\",\"!\"],[\"hi\",\"good\",\"to\",\"meet\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hi\",\".\"],[\"hi\",\"good\",\"to\",\"meet\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hello\", \",\", \"nice\", \"to\", \"meet\", \"you\"], [\"hi\", \",\", \"happy\", \"to\", \"meet\", \"you\", \"too\", \"!\"]))\n",
        "\tphrases.append(([\"hi\", \",\", \"nice\", \"to\", \"meet\", \"you\", \"!\"], [\"hi\", \",\", \"happy\", \"to\", \"meet\", \"you\", \"too\", \"!\"]))\n",
        "\tphrases.append(([\"hello\"],[\"it\",\"is\",\"my\",\"pleasure\"]))\n",
        "\tphrases.append(([\"hi\",\",\", \"how\", \"are\", \"you\",\"?\"],[\"i\",\"am\",\"fine\",\",\",\"thank\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hello\",\",\", \"how\", \"are\", \"you\",\"?\"],[\"i\",\"am\",\"great\",\",\",\"thank\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hi\",\"!\", \"how\", \"are\", \"you\",\"?\"],[\"not\",\"bad\",\",\",\"thank\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hello\", \"!\", \"how\", \"are\", \"you\",\"?\"],[\"i\",\"am\",\"fine\",\"thank\",\"you\",\".\"]))\n",
        "\tphrases.append(([\"hi\",\",\", \"how\", \"are\", \"you\",\"?\"],[\"i\",\"am\",\"good\",\",\",\"thank\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hello\",\",\", \"how\", \"are\", \"you\",\"?\"],[\"i\",\"am\",\"good\",\",\",\"thank\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hello\", \"!\", \"how\", \"are\", \"you\",\"?\"],[\"good\",\"enough\",\",\",\"thank\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hi\",\"!\", \"how\", \"are\", \"you\",\"?\"],[ \"hi\", \".\", \"i\",\"am\",\"fine\",\",\",\"thank\",\"you\",\"!\"]))\n",
        "\tphrases.append(([\"hello\",\".\", \"how\", \"are\", \"you\",\"?\"],[ \"hi\", \".\",\"i\",\"am\",\"great\",\",\",\"thank\",\"you\",\"!\"]))\n",
        "\n",
        "\tphrases = phrases * 12\n",
        "\tall_phrases = all_phrases + phrases\n",
        "\treturn all_phrases\n",
        "\n",
        "\n",
        "def calc_bleu_score(out, target):\n",
        "\tsf = bleu_score.SmoothingFunction()\n",
        "\treturn bleu_score.sentence_bleu(target, out, smoothing_function=sf.method1, weights=(0.5, 0.5))\n",
        "\n",
        "\n",
        "def get_words_from_toukens(sentence, token_dict):\n",
        "\treturn [token_dict[value] for value in sentence]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4B3MZuD9NCO",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "functions above are used to prepare and preprocces phrase_pairs dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZVOuTMl0Nkz",
        "colab_type": "code",
        "outputId": "3ca496a9-f633-43b6-f939-cd4c23a7e1fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "\n",
        "\n",
        "print(\"constructing vectorspace for pharse pairs dataset...\")\n",
        "\n",
        "all_representations, emb_token_dict = read_embeddings(EMB_FILE)\n",
        "phrase_pairs, dialogues = get_phrase_pairs(genres = GENRES.split(\",\"), exception = WITH_EXCEPTION)\n",
        "phrase_pairs = make_addition(phrase_pairs)\n",
        "word_dict = get_word_dict2(phrase_pairs, emb_token_dict)\n",
        "\n",
        "\n",
        "phrase_number = len(phrase_pairs)\n",
        "inverse_word_dict = {}\n",
        "\n",
        "for key in word_dict.keys():\n",
        "  inverse_word_dict[word_dict[key]] = key\n",
        "\n",
        "inverse_emb_dict = {}\n",
        "\n",
        "for key in emb_token_dict:\n",
        "    inverse_emb_dict[emb_token_dict[key]] = key\n",
        "\n",
        "print(\"vectorspace is constructed\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "constructing vectorspace for pharse pairs dataset...\n",
            "Total number of loaded phrases in comedy,romance,sci-fi movies is : 84623\n",
            "vectorspace is constructed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZ8fWUuq9XtH",
        "colab_type": "text"
      },
      "source": [
        "Code above constructs vectorspace for words which are present in phrase pairs dataset based on GLOVE 50-d embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UWZfYUz0PDW",
        "colab_type": "code",
        "outputId": "8b24484d-5f3c-4e60-a36c-88738a1d9880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "print(\"preparing and preprocces dataset...\")\n",
        "\n",
        "converted_x, converted_y, converted_x_reverse, true_lengths_x, true_lengths_y = convert_phrases2(phrase_pairs, word_dict)\n",
        "\n",
        "width_x = int(max(true_lengths_x))\n",
        "width_y = int(max(true_lengths_y))\n",
        "\n",
        "sparse_x = np.zeros((len(true_lengths_x), width_x))\n",
        "sparse_y = np.zeros((len(true_lengths_x), width_y))\n",
        "sparse_x_reverse = np.zeros((len(true_lengths_x), width_x))\n",
        "semi_hot_y = np.zeros((len(true_lengths_x), width_y))\n",
        "\n",
        "\n",
        "#padding each sequence in x_data and y_data\n",
        "for i in range(len(true_lengths_x)):\n",
        "    sparse_x[i, : true_lengths_x[i]] = np.array(converted_x[i])\n",
        "    sparse_y[i, : true_lengths_y[i]] = np.array(converted_y[i])\n",
        "    sparse_x_reverse[i, : true_lengths_x[i]] = np.array(converted_x_reverse[i])\n",
        "    semi_hot_y[i, : true_lengths_y[i]] = np.ones((true_lengths_y[i]))\n",
        "\n",
        "#semi_hot_y is needed to mask not needed losses of empty elements in sequence\n",
        "\n",
        "length = len(true_lengths_x)\n",
        "\n",
        "#lengths_x_one_hot is needed to mask hidden states retrieved from empty part of x sequence\n",
        "lengths_x_onehot = tf.one_hot(np.array(true_lengths_x) - 1, width_x)\n",
        "\n",
        "# and now we need to get rid of extra words in word_dict after we filtered phrase_pairs by lengths and unknown words \n",
        "# and convert phrases without extra tokens corresponding to those words\n",
        "unique_tokens = np.unique(np.concatenate((np.unique(sparse_x), np.unique(sparse_y)), axis = 0))\n",
        "transfer_tokens = {}\n",
        "transfer_dict = {}\n",
        "\n",
        "for key in word_dict.keys():\n",
        "    inverse_word_dict[word_dict[key]] = key\n",
        "\n",
        "\n",
        "for i in range(len(unique_tokens)):\n",
        "    transfer_tokens[unique_tokens[i]] = i\n",
        "    transfer_dict[inverse_word_dict[unique_tokens[i]]] = i\n",
        "\n",
        "inverse_word_dict = {}\n",
        "\n",
        "for key in transfer_dict.keys():\n",
        "    inverse_word_dict[transfer_dict[key]] = key\n",
        "\n",
        "word_dict = transfer_dict\n",
        "\n",
        "for i in range(length):\n",
        "    for j in range(width_x):\n",
        "        sparse_x[i,j] = transfer_tokens[sparse_x[i,j]]\n",
        "        sparse_x_reverse[i,j] = transfer_tokens[sparse_x_reverse[i,j]]\n",
        "\n",
        "    for k in range(width_y):\n",
        "        sparse_y[i,k] = transfer_tokens[sparse_y[i,k]]\n",
        "\n",
        "sparse_x = tf.convert_to_tensor(sparse_x, dtype = tf.int32)  \n",
        "sparse_y = tf.convert_to_tensor(sparse_y, dtype = tf.int32) \n",
        "sparse_x_reverse = tf.convert_to_tensor(sparse_x_reverse, dtype = tf.int32)  \n",
        "semi_hot_y = tf.convert_to_tensor(semi_hot_y, dtype = tf.int32)\n",
        "\n",
        "dict_size = len(list(word_dict.values()))    \n",
        "\n",
        "\n",
        "print(\"original dict_size:\", dict_size)\n",
        "print(\"number of phrase_pairs:\", length)\n",
        "print(\"min word frequency:\", MIN_TOKEN_FREQ)\n",
        "print(\"max phrase1 length:\", MAX_TOKENS_X)\n",
        "print(\"max phrase2 length:\", MAX_TOKENS_Y)\n",
        "print(\"data is ready\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preparing and preprocces dataset...\n",
            "original dict_size: 11137\n",
            "number of phrase_pairs: 61605\n",
            "min word frequency: 3\n",
            "max phrase1 length: 20\n",
            "max phrase2 length: 15\n",
            "data is ready\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkbZbjfF9gc6",
        "colab_type": "text"
      },
      "source": [
        "As well as TPU v1 can be executed only with static computational graph, recurrent layers have to be unrolled to train on TPU, as it needs to be unrolled all the training data must have fixed shape. \n",
        "\n",
        "In order to train seq2seq model with variable size input sequences, lenghts of train instances are encoded as one_hot for x_sequence vectors and semi_hot vectors for y_sequence (semi_hot means vector of ones of sequence lengths and zeroes for the rest).\n",
        "\n",
        "This vectors are multiplied with network output during training so non-zero gradient for each sequence will have variable lengths as training sequences and rest of the gradients will be zero.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BqQEsqP0Qkl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tpu_strategy.scope():\n",
        "\n",
        "\n",
        "\n",
        "    class Seq2Seq(tf.keras.Model):\n",
        "\n",
        "        ''' inherited from tf.keras.model class simle seq2seq model class with one embedding layer, 2 recurrent layers and two dense layers  '''\n",
        "\n",
        "        def __init__(   self, hidden_size, emb_size, \n",
        "                        word_dict, inverse_word_dict ):\n",
        "\n",
        "            super(Seq2Seq, self).__init__()\n",
        "            self.dict_size = len(word_dict)\n",
        "            self.hidden_size = hidden_size\n",
        "            self.emb_size = emb_size\n",
        "            self.word_dict = word_dict\n",
        "            self.inverse_word_dict = inverse_word_dict  \n",
        "\n",
        "            self.emb_layer = tf.keras.layers.Embedding(self.dict_size, emb_size)\n",
        "            self.decoder = tf.keras.layers.LSTM(units = hidden_size, recurrent_activation = \"sigmoid\", kernel_regularizer=regularizers.l2(0.003), recurrent_regularizer=regularizers.l2(0.003), bias_regularizer=regularizers.l2(0.003),\n",
        "                                                dropout = 0.05, recurrent_dropout = 0, return_state = True, return_sequences = True, unroll = True)\n",
        "            self.encoder = tf.keras.layers.GRU(units = hidden_size, recurrent_activation = \"sigmoid\", kernel_regularizer=regularizers.l2(0.003), recurrent_regularizer=regularizers.l2(0.003), bias_regularizer=regularizers.l2(0.003),\n",
        "                                                dropout = 0, recurrent_dropout = 0, return_state = True, unroll = True)\n",
        "            self.encoder_reverse = tf.keras.layers.GRU(units = hidden_size, recurrent_activation = \"sigmoid\", kernel_regularizer=regularizers.l2(0.003), recurrent_regularizer=regularizers.l2(0.003), bias_regularizer=regularizers.l2(0.003),\n",
        "                                                dropout = 0, recurrent_dropout = 0, return_state = True, unroll = True)\n",
        "            \n",
        "            self.interpreter = tf.keras.Sequential([ tf.keras.layers.Dense(self.dict_size) ])\n",
        "            self.end_words = [\".\"]\n",
        "\n",
        "\n",
        "        def get_words(self, tokens):\n",
        "\n",
        "            '''fucntion that returns words from output tokens'''\n",
        "\n",
        "            words = []\n",
        "            prev_token = None\n",
        "            for token in tokens:\n",
        "                if token != prev_token:\n",
        "                    words.append(self.inverse_word_dict[token])\n",
        "                    prev_token = token\n",
        "                    \n",
        "            return words\n",
        "\n",
        "\n",
        "\n",
        "        def encode_sequence(self, x_batch, x_batch_reverse, one_hot_x, seq_len, train):\n",
        "\n",
        "            ''' encode sequence method, that returns hidden state for encoder for each input x_sequence'''\n",
        "\n",
        "            batch_size = np.shape(x_batch)[0]\n",
        "            hidden_total = tf.zeros([batch_size, self.hidden_size])\n",
        "            hidden_state = tf.zeros([batch_size, self.hidden_size])\n",
        "            hidden_total_reverse = tf.zeros([batch_size, self.hidden_size])\n",
        "            hidden_state_reverse = tf.zeros([batch_size, self.hidden_size])\n",
        "            for i in range(seq_len):\n",
        "                _ , hidden_state = self.encoder(x_batch[:,i:i+1], hidden_state, training = train)\n",
        "                __, hidden_state_reverse = self.encoder_reverse(x_batch_reverse[:,i:i+1], hidden_state_reverse, training = train)\n",
        "                hidden_total += one_hot_x[:,i,None] * hidden_state\n",
        "                hidden_total_reverse += one_hot_x[:,i,None] * hidden_state_reverse\n",
        "            return [hidden_total, hidden_total_reverse]\n",
        "\n",
        "\n",
        "        def decode_sequence(self, hidden, y_batch, train ):\n",
        "\n",
        "            ''' method for training seq2seq with teacher \n",
        "                witch takes as argument encoded hidden state from x_input sequence\n",
        "            '''\n",
        "\n",
        "            output, _, _ = self.decoder(y_batch, hidden, training = train)\n",
        "            return self.interpreter(output)\n",
        "\n",
        "\n",
        "        def decode_chain_sequence_argmax(self, hidden, seq_len, train):\n",
        "\n",
        "            ''' method for training seq2seq without teacher \n",
        "                that takes as argument encoded hidden state and generates output as a chain sequence\n",
        "            '''\n",
        "\n",
        "            total_output = []\n",
        "            batch_size = np.shape(hidden[0])[0]\n",
        "            current_emb =  self.emb_layer(tf.ones((batch_size, 1)))\n",
        "\n",
        "            for i in range(seq_len):\n",
        "                output, hidden_h, hidden_c = self.decoder(current_emb, hidden, training = train)\n",
        "                hidden = [hidden_h, hidden_c]\n",
        "                current_distribution = self.interpreter(output)\n",
        "                total_output.append(current_distribution)\n",
        "                current_word = tf.argmax(current_distribution, axis=-1)\n",
        "                current_emb = self.emb_layer(current_word)\n",
        "\n",
        "            return tf.concat(total_output, axis = 1)\n",
        "\n",
        "\n",
        "        def decode_chain_sequence_sample(self, hidden, seq_len, train):\n",
        "\n",
        "            ''' method for training seq2seq without teacher \n",
        "                that takes as argument encoded hidden state and generates output as a chain sequence\n",
        "            '''\n",
        "\n",
        "            total_output = []\n",
        "            batch_size = np.shape(hidden[0])[0]\n",
        "            current_emb =  self.emb_layer(tf.ones((batch_size, 1)))\n",
        "            sampled_tokens = []\n",
        "\n",
        "            for i in range(seq_len):\n",
        "                output, hidden_h, hidden_c = self.decoder(current_emb, hidden, training = train)\n",
        "                hidden = [hidden_h, hidden_c]\n",
        "                current_distribution = self.interpreter(output)\n",
        "                total_output.append(current_distribution)\n",
        "                current_word = tf.random.categorical(current_distribution[:,0,:], 1)\n",
        "                sampled_tokens.append(current_word)\n",
        "                current_emb = self.emb_layer(current_word)\n",
        "\n",
        "            return tf.concat(total_output, axis = 1), tf.concat(sampled_tokens, axis = 1)\n",
        "\n",
        "\n",
        "        def call(self, input_x, input_x_reverse, input_y, one_hot_x, train = True):\n",
        "\n",
        "            ''' forward method for training with teacher'''\n",
        "\n",
        "            seq_len_x = np.shape(input_x)[1]\n",
        "            input_x_reverse = self.emb_layer(input_x_reverse)\n",
        "            input_x = self.emb_layer(input_x)\n",
        "            input_y = self.emb_layer(input_y)\n",
        "            hidden = self.encode_sequence(input_x, input_x_reverse, one_hot_x, seq_len_x, train)\n",
        "            predictions = self.decode_sequence(hidden, input_y, train)\n",
        "            return predictions\n",
        "\n",
        "\n",
        "        def call_2(self, input_x, input_x_reverse, one_hot_x, seq_len_y, train = True):\n",
        "\n",
        "            ''' forward method for training without teacher'''\n",
        "\n",
        "            seq_len_x = np.shape(input_x)[1]\n",
        "            input_x_reverse = self.emb_layer(input_x_reverse)\n",
        "            input_x = self.emb_layer(input_x)\n",
        "            hidden = self.encode_sequence(input_x, input_x_reverse, one_hot_x, seq_len_x, train)\n",
        "            predictions = self.decode_chain_sequence_argmax(hidden, seq_len_y, train)\n",
        "            return predictions\n",
        "\n",
        "\n",
        "        def call_sample(self, input_x, input_x_reverse, one_hot_x, seq_len_y, train = True):\n",
        "\n",
        "            ''' forward method for training without teacher'''\n",
        "\n",
        "            seq_len_x = np.shape(input_x)[1]\n",
        "            input_x_reverse = self.emb_layer(input_x_reverse)\n",
        "            input_x = self.emb_layer(input_x)\n",
        "            hidden = self.encode_sequence(input_x, input_x_reverse, one_hot_x, seq_len_x, train)\n",
        "            predictions, sampled_tokens = self.decode_chain_sequence_sample(hidden, seq_len_y, train)\n",
        "            return predictions, sampled_tokens\n",
        "\n",
        "\n",
        "\n",
        "        def save(self):\n",
        "            weights = []\n",
        "            weights.append(self.emb_layer.get_weights())\n",
        "            weights.append(self.decoder.get_weights())\n",
        "            weights.append(self.encoder.get_weights())\n",
        "            weights.append(self.encoder_reverse.get_weights())\n",
        "            for layer in self.interpreter.layers:\n",
        "                weights.append(layer.get_weights())\n",
        "            return weights\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        def decode_chain_sequence_test(self, hidden, r = 0, sampling = False):\n",
        "\n",
        "            ''' method applied to trained seq2seq that generate output sequence for each input phrase ''' \n",
        "\n",
        "            current_emb =  self.emb_layer(tf.ones((1, 1)))\n",
        "            total_output = []\n",
        "\n",
        "            if r > 0:\n",
        "                current_emb += tf.convert_to_tensor(np.random.rand(1, self.emb_size), dtype = tf.float32) * 0.1 * r\n",
        "\n",
        "            for _ in range(15):\n",
        "                output, hidden_h, hidden_c = self.decoder(current_emb, hidden, training = False)\n",
        "                hidden = [hidden_h, hidden_c]\n",
        "                current_distribution = self.interpreter(output)\n",
        "                if sampling:\n",
        "                    current_word = tf.random.categorical(current_distribution[0,:,:], 1)\n",
        "                else:\n",
        "                    current_word = tf.argmax(current_distribution, axis = -1 )\n",
        "                if self.inverse_word_dict[current_word.numpy()[0,0]] == \"END\"  or self.inverse_word_dict[current_word.numpy()[0,0]] == \"EMPTY\":\n",
        "                    if len(total_output) < 1:\n",
        "                        return self.decode_chain_sequence_test(hidden, r+1)\n",
        "                    else:\n",
        "                        return total_output\n",
        "\n",
        "                total_output.append(current_word.numpy()[0,0])  \n",
        "                current_emb = self.emb_layer(current_word)\n",
        "                r = 0\n",
        "\n",
        "            return total_output  \n",
        "\n",
        "\n",
        "        def encode_sequence_test(self, x, x_reverse):\n",
        "\n",
        "            ''' method applied to trained seq2seq model, returns hidden state for each input sequence'''\n",
        "\n",
        "            _ , hidden = self.encoder(x[None, :, :], training = False )\n",
        "            __, hidden_reverse = self.encoder_reverse(x_reverse[None, :, :], training = False )\n",
        "            return [hidden, hidden_reverse]  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        def predict(self, phrase, sampling = False):\n",
        "\n",
        "            ''' method applied on trained seq2seq that returns output phrase on each input phrase'''\n",
        "            \n",
        "            phrase_tokens = []\n",
        "\n",
        "            for word in phrase:\n",
        "                if word in self.word_dict.keys():\n",
        "                    phrase_tokens.append(self.word_dict[word])\n",
        "\n",
        "            phrase_tokens = phrase_tokens\n",
        "            phrase_tokens_reverse = [phrase_tokens[len(phrase_tokens)-1-i] for i in range(len(phrase_tokens))]\n",
        "\n",
        "            inputs = []\n",
        "            inputs_reverse = []\n",
        "\n",
        "            for word in phrase_tokens:\n",
        "                inputs.append(self.emb_layer(word))\n",
        "            \n",
        "            for word in phrase_tokens_reverse:\n",
        "                inputs_reverse.append(self.emb_layer(word))\n",
        "\n",
        "            inputs = tf.convert_to_tensor(inputs, dtype = tf.float32)\n",
        "            inputs_reverse = tf.convert_to_tensor(inputs_reverse, dtype = tf.float32)\n",
        "            hidden = self.encode_sequence_test(inputs, inputs_reverse)\n",
        "            output = self.decode_chain_sequence_test(hidden, sampling = sampling)\n",
        "            output_words = self.get_words(output)\n",
        "\n",
        "            return output_words\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFaWVdbO9wpS",
        "colab_type": "text"
      },
      "source": [
        "In code above class of seq2seq model for training is subclassed from tf.keras.model class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22NeQkmr0SSL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with tpu_strategy.scope():\n",
        "\n",
        "    def ngramm_recall_score( predictions, labels,\n",
        "                                semi_hot_y,\n",
        "                                seq_len,\n",
        "                                weights = tf.convert_to_tensor([1.,1.,1.,1.]) ):\n",
        "        \n",
        "        max_seq_len = np.shape(labels)[1]\n",
        "        batch_size = np.shape(labels)[0]\n",
        "        lengths = tf.where(tf.equal(predictions, 2), 1., 0.) * tf.range(0, max_seq_len, dtype = tf.float32)[None, :]\n",
        "        lengths  += tf.cast(tf.where(tf.equal(predictions, 2), 0., max_seq_len), dtype = tf.float32)\n",
        "        lengths = tf.reduce_min(lengths, axis = 1)\n",
        "        semihot_predictions = tf.ones((batch_size, max_seq_len), dtype = tf.float32) * tf.range(0, max_seq_len, dtype = tf.float32)[None, :]\n",
        "        semihot_predictions = tf.where(semihot_predictions <= lengths[:, None], 1., 0.)\n",
        "        predictions = tf.cast(predictions, tf.int32)\n",
        "        labels = tf.cast(labels, tf.int32)\n",
        "\n",
        "        n1_gramms = 0\n",
        "        for token_idx in range(max_seq_len):\n",
        "            n1_gramms += tf.math.sign( tf.reduce_sum( tf.where(tf.equal(labels[ :, token_idx : token_idx + 1], predictions), 1., 0.) * semi_hot_y[ :, token_idx : token_idx + 1] * semihot_predictions , axis = 1) )\n",
        "        n1_gramms /= seq_len\n",
        "            \n",
        "        n2_gramms = 0\n",
        "        for token_idx in range(max_seq_len-1):\n",
        "            n2_gramms += tf.math.sign( tf.reduce_sum( tf.where(tf.equal(labels[ :, token_idx : token_idx + 1], predictions[:,:-1]), 1., 0.) * semi_hot_y[:, token_idx : token_idx + 1] * \\\n",
        "                                                      tf.where(tf.equal(labels[ :, token_idx + 1 : token_idx + 2], predictions[:,1:]), 1., 0.) * semi_hot_y[:, token_idx + 1 : token_idx + 2] *\\\n",
        "                                                      semihot_predictions[:,1:] , axis = 1) )\n",
        "        n2_gramms /= tf.maximum((seq_len - 1.), 1.)\n",
        "\n",
        "        n3_gramms = 0\n",
        "        for token_idx in range(max_seq_len-2):\n",
        "            n3_gramms += tf.math.sign( tf.reduce_sum( tf.where(tf.equal(labels[ :, token_idx : token_idx + 1], predictions[:,:-2]), 1., 0.) * semi_hot_y[ :, token_idx : token_idx + 1] * \\\n",
        "                                                      tf.where(tf.equal(labels[ :, token_idx + 1 : token_idx + 2], predictions[:,1:-1]), 1., 0.) * semi_hot_y[ :, token_idx + 1 : token_idx + 2] * \\\n",
        "                                                      tf.where(tf.equal(labels[ :, token_idx + 2 : token_idx + 3], predictions[:,2:]), 1., 0.) * semi_hot_y[ :, token_idx + 2 : token_idx + 3]  * \\\n",
        "                                                      semihot_predictions[:,2:] , axis = 1) )      \n",
        "        n3_gramms /= tf.maximum((seq_len - 2.), 1.)\n",
        "\n",
        "        n4_gramms = 0\n",
        "        for token_idx in range(max_seq_len-3):\n",
        "            n4_gramms += tf.math.sign( tf.reduce_sum( tf.where(tf.equal(labels[ :, token_idx : token_idx + 1], predictions[:,:-3]), 1., 0.) * semi_hot_y[ :, token_idx : token_idx + 1] * \\\n",
        "                                                      tf.where(tf.equal(labels[ :, token_idx + 1 : token_idx + 2], predictions[:,1:-2]), 1., 0.) * semi_hot_y[ :, token_idx + 1 : token_idx + 2] * \\\n",
        "                                                      tf.where(tf.equal(labels[ :, token_idx + 2 : token_idx + 3], predictions[:,2:-1]), 1., 0.) * semi_hot_y[ :, token_idx + 2 : token_idx + 3] * \\\n",
        "                                                      tf.where(tf.equal(labels[ :, token_idx + 3 : token_idx + 4], predictions[:,3:]), 1., 0.) * semi_hot_y[ :, token_idx + 3 : token_idx + 4] * \\\n",
        "                                                      semihot_predictions[:,3:] , axis = 1) )\n",
        "        n4_gramms /= tf.maximum((seq_len - 3.), 1.)\n",
        "\n",
        "        labels_lengths = tf.reduce_sum(semi_hot_y, axis = 1)\n",
        "        predictions_lengths = tf.reduce_sum(semihot_predictions, axis = 1)\n",
        "        len_diff = tf.where(labels_lengths < predictions_lengths, predictions_lengths - labels_lengths, 0)\n",
        "\n",
        "        recall_penalty = 1 / (1.5/(1.5+tf.math.exp(-len_diff/2)))\n",
        "        recall_penalty /= tf.reduce_max(recall_penalty)\n",
        "\n",
        "        n_gramms = tf.stack([n1_gramms, n2_gramms, n3_gramms, n4_gramms], axis = 1)\n",
        "        weights /=tf.reduce_sum(weights)\n",
        "\n",
        "        return tf.reduce_sum(n_gramms*weights[None,:], axis = 1)*recall_penalty\n",
        "        \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JDieqCgc-tpp",
        "colab_type": "text"
      },
      "source": [
        "One of the most eficcient metrics to evaluate similarity between two sentences is Bleu score ot Bilingual evaluation understudy score, with calculates modified precision of two sencences based of n_gramms precision, in code above is implemented function which calculates BLEU score for multiple sentences in parallel."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JKw4vhGf0T2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "\n",
        "\n",
        "\tdef get_argmax_recall(labels, predictions, semi_hot_y, seq_lens):\n",
        "\n",
        "\t\tpredictions_argmax = tf.stop_gradient(tf.argmax(predictions, axis = -1))\n",
        "  \n",
        "\t\targmax_recall = ngramm_recall_score( predictions_argmax, labels,\n",
        "\t\t\t\t\t\t\t\t\t\t\tsemi_hot_y,\n",
        "\t\t\t\t\t\t\t\t\t\t\tseq_lens )\n",
        "  \n",
        "\t\treturn tf.nn.compute_average_loss(argmax_recall, global_batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "\n",
        "\tdef compute_loss1(labels, predictions, \n",
        "\t\t\t\t\t\tsemi_hot_y):\n",
        "\n",
        "\t\t''' cross_entropy_loss function for distributed strategy'''\n",
        "\n",
        "\t\tper_example_loss = loss_object(labels, predictions)*semi_hot_y\n",
        "\t\treturn tf.nn.compute_average_loss(per_example_loss , global_batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "\n",
        "\tdef compute_loss2(labels, predictions_probabilities_argmax, predictions_probabilities_sample,\n",
        "\t\t\t\t\t\tsampled_tokens,\n",
        "\t\t\t\t\t\tsemi_hot_y,\n",
        "\t\t\t\t\t\tseq_lens):\n",
        "\n",
        "\t\t'''log_probability loss for correcting output probalility distribution for distributed strategy'''\n",
        "\n",
        "\t\tprobabilities_no_grad = tf.stop_gradient(predictions_probabilities_argmax)\n",
        "\t\targmax_tokens = tf.stop_gradient(tf.cast(tf.argmax(probabilities_no_grad, axis = -1), tf.float32))\n",
        "\t\tsampled_tokens = tf.stop_gradient(tf.cast(sampled_tokens, tf.float32))\n",
        "\t\tlabels = tf.cast(labels, tf.float32)\n",
        "\t\tbatch_size = np.shape(labels)[0]\n",
        "\t\tmax_seq_len = np.shape(labels)[1]\n",
        "\t\tsemihot_sampled = []\n",
        "\t\tsample_recall = []\n",
        "        \n",
        "\t\tfor idx in range(np.shape(sampled_tokens)[0]):\n",
        "\t\t\tsampled_lengths = tf.where(tf.equal(sampled_tokens[idx, :, :], 2), 1., 0.) * tf.range(0, max_seq_len, dtype = tf.float32)[None, :]\n",
        "\t\t\tsampled_lengths  += tf.cast(tf.where(tf.equal(sampled_tokens[idx, :, :], 2), 0., max_seq_len), dtype = tf.float32)\n",
        "\t\t\tsampled_lengths = tf.reduce_min(sampled_lengths, axis = 1)\n",
        "\t\t\tsemihot_sampled_ = tf.ones((batch_size, max_seq_len), dtype = tf.float32) * tf.range(0, max_seq_len, dtype = tf.float32)[None, :]\n",
        "\t\t\tsemihot_sampled.append(tf.where(semihot_sampled_ <= sampled_lengths[:, None], 1., 0.))\n",
        "\t\t\tsample_recall.append(ngramm_recall_score( sampled_tokens[idx, :, :], labels,\n",
        "                                \t\t\t\t\t\tsemi_hot_y,\n",
        "                                \t\t\t\t\t\tseq_lens ))\n",
        "  \n",
        "\t\targmax_recall = ngramm_recall_score( argmax_tokens, labels,\n",
        "                                \t\t\t\t\t\tsemi_hot_y,\n",
        "                                \t\t\t\t\t\tseq_lens )\n",
        "\t\tlog_probabilities_corrector = []\n",
        "\t\tfor idx in range(np.shape(sampled_tokens)[0]):\n",
        "\t\t\trecall_corrector = tf.where( argmax_recall < 0.97, sample_recall[idx] - argmax_recall , 0. )\n",
        "\t\t\tgathered_sample_probabilities = tf.gather_nd( - tf.nn.log_softmax(predictions_probabilities_sample[idx,:,:], axis = -1) , tf.cast(sampled_tokens[idx,:,:], tf.int32)[:,:,None], batch_dims = 2)\n",
        "\t\t\tlog_probabilities_corrector.append(gathered_sample_probabilities  * recall_corrector[:, None] * semihot_sampled[idx] )\n",
        "   \n",
        "\t\tlog_probabilities_corrector = tf.concat(log_probabilities_corrector, axis = 0)\n",
        "\t\tsample_recall = tf.concat(sample_recall, axis = 0)\n",
        "\t\treturn tf.nn.compute_average_loss(log_probabilities_corrector, global_batch_size=BATCH_SIZE*np.shape(sampled_tokens)[0]),  \\\n",
        "               tf.nn.compute_average_loss(sample_recall, global_batch_size=BATCH_SIZE*np.shape(sampled_tokens)[0]), \\\n",
        "               tf.nn.compute_average_loss(argmax_recall, global_batch_size=BATCH_SIZE)\n",
        "\n",
        "\n",
        "\n",
        "\t### For each training method corresponding distributed training step function needs to be constructed \n",
        "\n",
        "\tdef distributed_train_step1(dataset_inputs):\n",
        "\n",
        "\t\t''' function that applies distriuted TPU strategy to train_step1 function'''\n",
        "\n",
        "\t\tper_replica_losses = tpu_strategy.experimental_run_v2(train_step1,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\targs=(dataset_inputs,))\n",
        "\t\treturn tpu_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\taxis=None)\n",
        "\n",
        "\tdef distributed_train_step2(dataset_inputs):\n",
        "\n",
        "\t\t''' function that applies distriuted TPU strategy to train_step2 function'''\n",
        "\n",
        "\t\tper_replica_losses = tpu_strategy.experimental_run_v2(train_step2,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\targs=(dataset_inputs,))\n",
        "\t\treturn tpu_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\taxis=None)\n",
        "\n",
        "\tdef distributed_correction(dataset_inputs):\n",
        "\n",
        "\t\t''' function that applies distriuted TPU strategy to train_step3 function'''\n",
        "\n",
        "\t\tper_replica_losses = tpu_strategy.experimental_run_v2(dist_correction,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\targs=(dataset_inputs,))\n",
        "\t\treturn tpu_strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\taxis=None)\n",
        "\n",
        "\n",
        "\t@tf.function\n",
        "\tdef train_step1(inputs):\n",
        "\n",
        "\t\t''' train_step1 fuction for crossentropy teacher training method \n",
        "\t\t\twith @tf.function decorator that assures construction of static computation graph for training\n",
        "\t\t\tfunction returns crosscntropy_loss regardless wich loss was applied at training step in order to track loss value during training loop\n",
        "\t\t'''\n",
        "\t\tinput_x = inputs[0]\n",
        "\t\tinput_x_reverse = inputs[1]\n",
        "\t\tinput_y = inputs[2]\n",
        "\t\tone_hot_x = inputs[3]\n",
        "\t\tsemi_hot_y = tf.cast(inputs[4], tf.float32)\n",
        "\t\twith tf.GradientTape() as tape:\n",
        "\t\t\tpredictions = seq2seq(input_x, input_x_reverse, input_y[:,:-1], one_hot_x)\n",
        "\t\t\tloss = compute_loss1(input_y[:,1:], predictions, semi_hot_y[:,1:])\n",
        "\t\tgradients = tape.gradient(loss, seq2seq.trainable_variables)\n",
        "\t\toptimizer1.apply_gradients(zip(gradients, seq2seq.trainable_variables))\n",
        "\t\treturn loss\n",
        "\n",
        "\n",
        "\t@tf.function\n",
        "\tdef train_step2(inputs):\n",
        "\n",
        "\t\t''' train_step2 fuction for crossentropy chain_sequence training method \n",
        "\t\t'''\n",
        "\t\tinput_x = inputs[0]\n",
        "\t\tinput_x_reverse = inputs[1]\n",
        "\t\tinput_y = inputs[2]\n",
        "\t\tone_hot_x = inputs[3]\n",
        "\t\tsemi_hot_y = tf.cast(inputs[4], tf.float32)\n",
        "\t\tseq_lens = tf.reduce_sum(semi_hot_y[:,1:], axis = 1)\n",
        "\t\twith tf.GradientTape() as tape:\n",
        "\t\t\tpredictions = seq2seq.call_2(input_x, input_x_reverse, one_hot_x, np.shape(input_y)[1]-1)\n",
        "\t\t\tloss = compute_loss1(input_y[:,1:], predictions, semi_hot_y[:,1:])\n",
        "\t\t\targmax_recall = get_argmax_recall(input_y[:,1:], predictions, semi_hot_y[:,1:], seq_lens)\n",
        "\t\tgradients = tape.gradient(loss, seq2seq.trainable_variables)\n",
        "\t\toptimizer2.apply_gradients(zip(gradients, seq2seq.trainable_variables))\n",
        "\n",
        "\t\ttrain_accuracy(input_y[:,1:], tf.keras.activations.softmax(predictions, axis = -1) *\n",
        "\t\t\t\t\tsemi_hot_y[:,1:,None] + (1-semi_hot_y)[:,1:,None]*tf.one_hot(0,seq2seq.dict_size)[None, None,:]) # in order to get correct accuracy we need to mask padded values\n",
        "\n",
        "\t\treturn tf.stack([loss, argmax_recall])\n",
        "\n",
        "\t@tf.function\n",
        "\tdef dist_correction(inputs):\n",
        "\n",
        "\t\t''' dist_correction fuction performs output distribution correction based on word2wec \n",
        "\t\t'''\n",
        "\n",
        "\t\tinput_x = inputs[0]\n",
        "\t\tinput_x_reverse = inputs[1]\n",
        "\t\tinput_y = inputs[2]\n",
        "\t\tone_hot_x = inputs[3]\n",
        "\t\tsemi_hot_y = tf.cast(inputs[4], tf.float32)\n",
        "\t\tseq_lens = tf.reduce_sum(semi_hot_y[:,1:], axis = 1)\n",
        "\t\tpredictions_sample = []\n",
        "\t\tsampled_tokens = []\n",
        "\t\twith tf.GradientTape() as tape:\n",
        "\t\t\tpredictions_argmax = seq2seq.call_2(input_x, input_x_reverse, one_hot_x, np.shape(input_y)[1]-1, train = False)\n",
        "\t\t\tfor _ in range(15):\n",
        "\t\t\t\tsampled_values = seq2seq.call_sample(input_x, input_x_reverse, one_hot_x, np.shape(input_y)[1]-1, train = False)\n",
        "\t\t\t\tpredictions_sample.append(sampled_values[0])\n",
        "\t\t\t\tsampled_tokens.append(sampled_values[1])\n",
        "\t\t\tpredictions_sample = tf.stack(predictions_sample, axis = 0)\n",
        "\t\t\tsampled_tokens = tf.stack(sampled_tokens, axis = 0)\n",
        "\t\t\tloss, sample_recall, argmax_recall = compute_loss2(input_y[:,1:], predictions_argmax, predictions_sample, sampled_tokens, semi_hot_y[:,1:], seq_lens)\n",
        "\t\t\tprint_loss = tf.stop_gradient(compute_loss1(input_y[:,1:], predictions_argmax, semi_hot_y[:,1:]))\n",
        "\t\tgradients = tape.gradient(loss, seq2seq.trainable_variables, )\n",
        "\t\toptimizer3.apply_gradients(zip(gradients, seq2seq.trainable_variables))\n",
        "  \n",
        "\t\ttrain_accuracy(input_y[:,1:], tf.keras.activations.softmax(predictions_argmax, axis = -1) *\n",
        "\t\t                 semi_hot_y[:,1:,None] + (1-semi_hot_y)[:,1:,None]*tf.one_hot(0,seq2seq.dict_size)[None, None,:]) # in order to get correct accuracy we need to mask padded values\n",
        "\n",
        "\t\treturn tf.stack([print_loss, sample_recall, argmax_recall], axis = 0)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCODAm1dAZgg",
        "colab_type": "text"
      },
      "source": [
        "Above are defined 3 different loss function for training seq2seq model:\n",
        "  1. cross_entropy los function for teacher training method\n",
        "  2. cross_entropy loss function for chain training method\n",
        "  2. probability correction loss function for correcting output probability distribution over, this loss function is applied to sampled sequence rather than argmax and is baed on calculating recall score between labels and sampled sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpKMVV_40Vvd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " with tpu_strategy.scope():\n",
        "\n",
        "    def crossentropy_training_loop( train_dist_dataset, \n",
        "                                    teacher_prob,\n",
        "                                    mean_pre_loss, argmax_recals ):\n",
        "\n",
        "        ''' training function for crossentropy training loop\n",
        "        '''\n",
        "\n",
        "        for x in train_dist_dataset:\n",
        "            if np.random.rand() < teacher_prob:\n",
        "                loss = distributed_train_step1(x)\n",
        "            else:\n",
        "                track_values = distributed_train_step2(x)\n",
        "                loss2 = track_values[0]\n",
        "                argmax_recal = track_values[1]\n",
        "                mean_pre_loss.append(loss2)\n",
        "                argmax_recals.append(argmax_recal)\n",
        "        \n",
        "            if teacher_prob == 1:\n",
        "                mean_pre_loss.append(loss)\n",
        "\n",
        "\n",
        "    def correction_training_loop( train_dist_dataset, \n",
        "                                    teacher_prob,\n",
        "                                    mean_pre_loss, mean_sample_bleu, mean_argmax_bleu ):\n",
        "\n",
        "        ''' training function for crossentropy training loop\n",
        "        '''\n",
        "\n",
        "        for x in train_dist_dataset:\n",
        "\n",
        "\n",
        "            track_values = distributed_correction(x)\n",
        "            loss = track_values[0]\n",
        "            sample_bleu = track_values[1]\n",
        "            argmax_bleu = track_values[2]\n",
        "\n",
        "            mean_pre_loss.append(loss)\n",
        "            mean_sample_bleu.append(sample_bleu)\n",
        "            mean_argmax_bleu.append(argmax_bleu)\n",
        "\n",
        "\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "\t\t\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VX3cIvQBczc",
        "colab_type": "text"
      },
      "source": [
        "Training circle functions for cross_entropy training method and probability correction is defined in code above"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXR7oFz00YZb",
        "colab_type": "code",
        "outputId": "1efc1d24-2055-41b1-ac16-fc05ea855676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EMBEDDING_SIZE = 128\n",
        "HIDDEN_SIZE = 1024\n",
        "BATCH_SIZE = 1024\n",
        "\n",
        "\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "tf.autograph.set_verbosity(1)\n",
        "\n",
        "with tpu_strategy.scope():\n",
        "\n",
        "\tdef relu(x):\n",
        "\t\tif x < 0:\n",
        "\t\t\treturn 0\n",
        "\t\telse:\n",
        "\t\t\treturn x\n",
        "\n",
        "\n",
        "\t\n",
        "\t#instanciating seq2seq model from seq2seq class\n",
        "\tseq2seq = Seq2Seq( HIDDEN_SIZE, EMBEDDING_SIZE,\n",
        "\t\t\t\t\t   word_dict, inverse_word_dict )\n",
        "\n",
        "\t\n",
        "\ttrain_length = np.shape(sparse_x)[0]\n",
        "\ttrain_steps = int(train_length/BATCH_SIZE)\n",
        "\n",
        "\t#defining tf_callbacks and evaluation metrices\n",
        "\ttrain_accuracy = tf.keras.metrics.SparseCategoricalAccuracy('train_accuracy')\n",
        "\tnull_train_acc = np.mean(np.sum(1-semi_hot_y, axis = 1)-1)/np.shape(sparse_y)[1]\n",
        "\n",
        "\ttrain_losses = []\n",
        "\ttrain_accuracies = []\n",
        "\tteacher_method_losses=[]\n",
        "\n",
        "\ttrain_correction_accuracies = []\n",
        "\tcorrection_sample_recall = []\n",
        "\tcorrection_argmax_recall = []\n",
        "\t\n",
        "\n",
        "\tLEARNING_RATE = tf.keras.optimizers.schedules.ExponentialDecay(0.005, decay_steps = train_steps, decay_rate = 0.99, staircase= True)\n",
        "\tLEARNING_RATE2 = tf.keras.optimizers.schedules.ExponentialDecay(7e-5, decay_steps = train_steps*4, decay_rate = 1.1, staircase= True)    \n",
        "\n",
        "\tloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction=tf.keras.losses.Reduction.NONE)\n",
        "\n",
        "\toptimizer1 = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\toptimizer2 = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
        "\toptimizer3 = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE2)\n",
        "\toptimizer4 = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE2)\n",
        "\n",
        "\tn_epochs = 450\n",
        "\tis_cross_entropy = True\n",
        "\tteacher_prob = 1\n",
        "\tprint(\"crossentropy training ... \")\n",
        "\tfor epoch in range(n_epochs):\n",
        "\t\tepoch_start = time.localtime(time.time())\n",
        "\t\t\n",
        "\t\tmean_pre_loss = [0]\n",
        "\t\tmean_sample_recall = []\n",
        "\t\tmean_argmax_recall = []\n",
        "\n",
        "\t\t# in order to make training independent from instances order in batches, distributed training dataset have to be shuffled and reconstructed every epoch during training\n",
        "\t\tdataset = tf.data.Dataset.from_tensor_slices((sparse_x, sparse_x_reverse, sparse_y, lengths_x_onehot, semi_hot_y)).shuffle(150000).batch(BATCH_SIZE, drop_remainder = True) \n",
        "\t\ttrain_dist_dataset = tpu_strategy.experimental_distribute_dataset(dataset)\n",
        "  \n",
        "\t\tif epoch == 399:\n",
        "\t\t\tprint(\"distribution correction training...\")\n",
        "\t\t\tteacher_prob = 0\n",
        "\t\t\tis_cross_entropy = False\n",
        "\n",
        "\t\tif(epoch + 1) % 40 == 0 and is_cross_entropy:\n",
        "\t\t\tteacher_prob = 1 - teacher_prob\n",
        "\n",
        "\t\tif is_cross_entropy:    \n",
        "\n",
        "\t\t\tcrossentropy_training_loop( train_dist_dataset, \n",
        "\t\t\t\t\t\t\t\t\t\tteacher_prob, \n",
        "\t\t\t\t\t\t\t\t\t\tmean_pre_loss, mean_argmax_recall)\n",
        "\t\telse:\n",
        "\n",
        "\t\t\tcorrection_training_loop( train_dist_dataset, \n",
        "\t\t\t\t\t\t\t\t\t\tteacher_prob, \n",
        "\t\t\t\t\t\t\t\t\t\tmean_pre_loss, mean_sample_recall, mean_argmax_recall)\n",
        "\n",
        "\t\tepoch_end = time.localtime(time.time())\n",
        "\t\tstart_in_sec = epoch_start[3]*3600 + epoch_start[4]*60 + epoch_start[5]\n",
        "\t\tend_in_sec = epoch_end[3]*3600 + epoch_end[4]*60 + epoch_end[5]\n",
        "\t\tepoch_time = end_in_sec - start_in_sec\n",
        "\n",
        "\t\ttrain_loss = np.mean(mean_pre_loss)\n",
        "\t\ttrain_acc = relu((train_accuracy.result().numpy()-null_train_acc)/(1-null_train_acc))\n",
        "\t\ttrain_accuracy.reset_states()\n",
        "\n",
        "\t\tif is_cross_entropy:\n",
        "\t\t\tif not teacher_prob == 1:\n",
        "\t\t\t\targmax_recall = np.mean(mean_argmax_recall)\n",
        "\t\t\t\tcorrection_argmax_recall.append(argmax_recall)\n",
        "\t\t\t\ttrain_losses.append(train_loss)\n",
        "\t\t\t\ttrain_accuracies.append(train_acc)\n",
        "\t\n",
        "\t\t\t\tprint(\"Epoch: \", epoch,\", Train loss:\", train_loss, \", Train accuracy:\", train_acc, \", argmax_recall_score:\", argmax_recall, \", teacher_prob:\", teacher_prob,\", time: \", epoch_time ,\" sec \")\n",
        "\n",
        "\t\t\telse:\n",
        "\t\t\t\tteacher_method_losses.append(train_loss)\n",
        "\t\t\t\tprint(\"Epoch: \", epoch,\", Train loss:\", train_loss, \", Train accuracy:\", train_acc, \", teacher_prob:\", teacher_prob,\", time: \", epoch_time ,\" sec \")\n",
        "\t\n",
        "\t\n",
        "\n",
        "\t\telse:\n",
        "\t\t\tsample_recall = np.mean(mean_sample_recall)\n",
        "\t\t\targmax_recall = np.mean(mean_argmax_recall)\n",
        "\t\t\tprint(\"Epoch: \", epoch,\", Train loss:\", train_loss, \", Train accuracy:\", train_acc, \", sample_recall_score:\", sample_recall, \", argmax_recall_score:\", argmax_recall, \", time: \", epoch_time ,\" sec \")\t\n",
        "\n",
        "\t\t\ttrain_correction_accuracies.append(train_acc)\n",
        "\t\t\tcorrection_sample_recall.append(sample_recall)\n",
        "\t\t\tcorrection_argmax_recall.append(argmax_recall)\n",
        "   \n",
        "\n",
        "\t\n",
        "\t\t\t\t\n",
        "\t\t\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "crossentropy training ... \n",
            "Epoch:  0 , Train loss: 42.73981582141313 , Train accuracy: 0 , teacher_prob: 1 , time:  45  sec \n",
            "Epoch:  1 , Train loss: 34.211552291619974 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  2 , Train loss: 31.24549159065622 , Train accuracy: 0 , teacher_prob: 1 , time:  8  sec \n",
            "Epoch:  3 , Train loss: 29.4046763435739 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  4 , Train loss: 27.99620418861264 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  5 , Train loss: 26.764858277117618 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  6 , Train loss: 25.57627152614906 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  7 , Train loss: 24.430249042198305 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  8 , Train loss: 23.330147289838948 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  9 , Train loss: 22.28549163067927 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  10 , Train loss: 21.284809487764953 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  11 , Train loss: 20.344692574172722 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  12 , Train loss: 19.47300642044818 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  13 , Train loss: 18.66440697967029 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  14 , Train loss: 17.88758087158203 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  15 , Train loss: 17.176672263223615 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  16 , Train loss: 16.517218011324523 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  17 , Train loss: 15.894874838531994 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  18 , Train loss: 15.32999714085313 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  19 , Train loss: 14.810182446339091 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  20 , Train loss: 14.326197921252641 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  21 , Train loss: 13.891007642276952 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  22 , Train loss: 13.488669895734944 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  23 , Train loss: 13.122905324717037 , Train accuracy: 0 , teacher_prob: 1 , time:  8  sec \n",
            "Epoch:  24 , Train loss: 12.786533637124984 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  25 , Train loss: 12.487812261112401 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  26 , Train loss: 12.20659529576536 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  27 , Train loss: 11.968739681556576 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  28 , Train loss: 11.719282181536565 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  29 , Train loss: 11.512453782753866 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  30 , Train loss: 11.310431011387559 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  31 , Train loss: 11.127440155529586 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  32 , Train loss: 10.96742895782971 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  33 , Train loss: 10.799052175928335 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  34 , Train loss: 10.659663919542657 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  35 , Train loss: 10.521895095950267 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  36 , Train loss: 10.391768893257517 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  37 , Train loss: 10.262390824614979 , Train accuracy: 0 , teacher_prob: 1 , time:  8  sec \n",
            "Epoch:  38 , Train loss: 10.149252954076548 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  39 , Train loss: 48.182250601346375 , Train accuracy: 0.26875989001838463 , argmax_recall_score: 0.08563288 , teacher_prob: 0 , time:  50  sec \n",
            "Epoch:  40 , Train loss: 40.24759611536245 , Train accuracy: 0.2896263028779685 , argmax_recall_score: 0.0888737 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  41 , Train loss: 39.85929789308642 , Train accuracy: 0.2906965967619212 , argmax_recall_score: 0.088314444 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  42 , Train loss: 39.61522193033187 , Train accuracy: 0.29155747358739004 , argmax_recall_score: 0.089154646 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  43 , Train loss: 39.39358708115875 , Train accuracy: 0.2924132229333341 , argmax_recall_score: 0.09108391 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  44 , Train loss: 39.17373963653064 , Train accuracy: 0.2929107233809052 , argmax_recall_score: 0.091912344 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  45 , Train loss: 38.936402805515975 , Train accuracy: 0.29380169041516385 , argmax_recall_score: 0.09446979 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  46 , Train loss: 38.67307150168497 , Train accuracy: 0.29517504532576283 , argmax_recall_score: 0.09537463 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  47 , Train loss: 38.39600678740955 , Train accuracy: 0.29603079467170684 , argmax_recall_score: 0.0978793 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  48 , Train loss: 38.115037699214746 , Train accuracy: 0.29709434187207445 , argmax_recall_score: 0.098544054 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  49 , Train loss: 37.78502042176294 , Train accuracy: 0.29837553708489983 , argmax_recall_score: 0.09978919 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  50 , Train loss: 37.410167756627814 , Train accuracy: 0.3000403487263785 , argmax_recall_score: 0.101564296 , teacher_prob: 0 , time:  7  sec \n",
            "Epoch:  51 , Train loss: 37.017954591844905 , Train accuracy: 0.3008978522100547 , argmax_recall_score: 0.10369266 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  52 , Train loss: 36.56787497098328 , Train accuracy: 0.3032376020773947 , argmax_recall_score: 0.10526087 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  53 , Train loss: 36.095772164766906 , Train accuracy: 0.3052188331790182 , argmax_recall_score: 0.106672116 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  54 , Train loss: 35.578109803746955 , Train accuracy: 0.30739113035977406 , argmax_recall_score: 0.10840471 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  55 , Train loss: 35.06041479892418 , Train accuracy: 0.30997200669844827 , argmax_recall_score: 0.11109277 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  56 , Train loss: 34.49994321729316 , Train accuracy: 0.31366163301751077 , argmax_recall_score: 0.11321953 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  57 , Train loss: 33.917286231869554 , Train accuracy: 0.31761586426678395 , argmax_recall_score: 0.11593496 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  58 , Train loss: 33.301700279360915 , Train accuracy: 0.32203561668343444 , argmax_recall_score: 0.11830536 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  59 , Train loss: 32.67823807137911 , Train accuracy: 0.32785233740323205 , argmax_recall_score: 0.12175485 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  60 , Train loss: 32.04704390979204 , Train accuracy: 0.3338447417635877 , argmax_recall_score: 0.124741726 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  61 , Train loss: 31.411709300807264 , Train accuracy: 0.3403396391173675 , argmax_recall_score: 0.1283297 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  62 , Train loss: 30.757649937614065 , Train accuracy: 0.3481409642805812 , argmax_recall_score: 0.13261396 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  63 , Train loss: 30.142560802522254 , Train accuracy: 0.35489210067692606 , argmax_recall_score: 0.13593963 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  64 , Train loss: 29.463795052200066 , Train accuracy: 0.36364308902158954 , argmax_recall_score: 0.1408004 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  65 , Train loss: 28.851316608366417 , Train accuracy: 0.3721611818488927 , argmax_recall_score: 0.1456401 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  66 , Train loss: 28.247737978325514 , Train accuracy: 0.38059723500380005 , argmax_recall_score: 0.14987464 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  67 , Train loss: 27.630410147494956 , Train accuracy: 0.3894335014289802 , argmax_recall_score: 0.15527214 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  68 , Train loss: 27.005346548361857 , Train accuracy: 0.39924115535675436 , argmax_recall_score: 0.16069077 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  69 , Train loss: 26.46446162364522 , Train accuracy: 0.40775600977593657 , argmax_recall_score: 0.16579357 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  70 , Train loss: 25.842925087350313 , Train accuracy: 0.41765906180960516 , argmax_recall_score: 0.17109169 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  71 , Train loss: 25.24942404324891 , Train accuracy: 0.428088220229246 , argmax_recall_score: 0.17826511 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  72 , Train loss: 24.70072858841693 , Train accuracy: 0.4375994248802874 , argmax_recall_score: 0.18510413 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  73 , Train loss: 24.16943515715052 , Train accuracy: 0.44796152126509153 , argmax_recall_score: 0.19104862 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  74 , Train loss: 23.57997037543625 , Train accuracy: 0.45889128360675263 , argmax_recall_score: 0.19863111 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  75 , Train loss: 23.048841101224305 , Train accuracy: 0.46904909041259757 , argmax_recall_score: 0.20509173 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  76 , Train loss: 22.550405345979286 , Train accuracy: 0.47899586095589813 , argmax_recall_score: 0.21294482 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  77 , Train loss: 22.1193706012163 , Train accuracy: 0.4872258703941141 , argmax_recall_score: 0.21897583 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  78 , Train loss: 21.601418854760343 , Train accuracy: 0.49782909325025254 , argmax_recall_score: 0.22771026 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  79 , Train loss: 21.764662883320792 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  80 , Train loss: 18.370848671334688 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  81 , Train loss: 16.210054116170916 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  82 , Train loss: 14.698356143763808 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  83 , Train loss: 13.51522111111 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  84 , Train loss: 12.554272979986472 , Train accuracy: 0 , teacher_prob: 1 , time:  9  sec \n",
            "Epoch:  85 , Train loss: 11.76485869923576 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  86 , Train loss: 11.080770664527767 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  87 , Train loss: 10.460961326223906 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  88 , Train loss: 9.928380090682232 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  89 , Train loss: 9.455705783406241 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  90 , Train loss: 9.035562859206903 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  91 , Train loss: 8.651079725046626 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  92 , Train loss: 8.313530921936035 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  93 , Train loss: 8.032048092513788 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  94 , Train loss: 7.737909911108798 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  95 , Train loss: 7.484462386271993 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  96 , Train loss: 7.22984417149278 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  97 , Train loss: 6.9946757613635455 , Train accuracy: 0 , teacher_prob: 1 , time:  8  sec \n",
            "Epoch:  98 , Train loss: 6.7887662746867194 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  99 , Train loss: 6.61540057229214 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  100 , Train loss: 6.438580075248343 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  101 , Train loss: 6.2946546351323365 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  102 , Train loss: 6.170798020284684 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  103 , Train loss: 6.0398132527460815 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  104 , Train loss: 5.920620769750877 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  105 , Train loss: 5.8010758571937435 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  106 , Train loss: 5.678718027521352 , Train accuracy: 0 , teacher_prob: 1 , time:  8  sec \n",
            "Epoch:  107 , Train loss: 5.558568000793457 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  108 , Train loss: 5.461187917678083 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  109 , Train loss: 5.361886133913134 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  110 , Train loss: 5.257893570133897 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  111 , Train loss: 5.180492401123047 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  112 , Train loss: 5.113247621254843 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  113 , Train loss: 5.043766467297663 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  114 , Train loss: 4.952834340392566 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  115 , Train loss: 4.865169337538422 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  116 , Train loss: 4.7898970431968815 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  117 , Train loss: 4.715790115418981 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  118 , Train loss: 4.654076263552806 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  119 , Train loss: 29.762431629368518 , Train accuracy: 0.3857020456716896 , argmax_recall_score: 0.17995498 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  120 , Train loss: 24.714783965564166 , Train accuracy: 0.4263849524913318 , argmax_recall_score: 0.19518642 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  121 , Train loss: 22.385720893984935 , Train accuracy: 0.46791861611106506 , argmax_recall_score: 0.22814697 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  122 , Train loss: 20.67096119239682 , Train accuracy: 0.5027899296904343 , argmax_recall_score: 0.25819093 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  123 , Train loss: 19.327035403642498 , Train accuracy: 0.5322765801668009 , argmax_recall_score: 0.2858654 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  124 , Train loss: 18.252435215183947 , Train accuracy: 0.5573938084860625 , argmax_recall_score: 0.31038302 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  125 , Train loss: 17.38076361671823 , Train accuracy: 0.5773743817909124 , argmax_recall_score: 0.33008242 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  126 , Train loss: 16.661076233035228 , Train accuracy: 0.5948157732616445 , argmax_recall_score: 0.3493237 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  127 , Train loss: 16.01843702597696 , Train accuracy: 0.6105524127240736 , argmax_recall_score: 0.36607805 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  128 , Train loss: 15.370199453635294 , Train accuracy: 0.626173279096181 , argmax_recall_score: 0.38446876 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  129 , Train loss: 14.801557806671642 , Train accuracy: 0.6410072122949133 , argmax_recall_score: 0.4003832 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  130 , Train loss: 14.338087128811194 , Train accuracy: 0.6521798552456408 , argmax_recall_score: 0.41479138 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  131 , Train loss: 13.90144986011943 , Train accuracy: 0.6627428678676116 , argmax_recall_score: 0.42744696 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  132 , Train loss: 13.559540701694177 , Train accuracy: 0.6714821169828369 , argmax_recall_score: 0.43871784 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  133 , Train loss: 13.14726695076364 , Train accuracy: 0.6814622161430481 , argmax_recall_score: 0.4496812 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  134 , Train loss: 12.825329139584401 , Train accuracy: 0.6892870197651383 , argmax_recall_score: 0.4606608 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  135 , Train loss: 12.48828628414967 , Train accuracy: 0.6975019166321236 , argmax_recall_score: 0.47162503 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  136 , Train loss: 12.22712863859583 , Train accuracy: 0.7042866515127225 , argmax_recall_score: 0.47961524 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  137 , Train loss: 11.992734690181544 , Train accuracy: 0.7096325886519464 , argmax_recall_score: 0.48820907 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  138 , Train loss: 11.720114942456854 , Train accuracy: 0.716075671475792 , argmax_recall_score: 0.49689028 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  139 , Train loss: 11.483836986979501 , Train accuracy: 0.7215840687557469 , argmax_recall_score: 0.5049644 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  140 , Train loss: 11.295831008035629 , Train accuracy: 0.7265498977417818 , argmax_recall_score: 0.51236016 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  141 , Train loss: 11.08164993661349 , Train accuracy: 0.7312393825681678 , argmax_recall_score: 0.51917154 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  142 , Train loss: 10.881143397972233 , Train accuracy: 0.7362420484465777 , argmax_recall_score: 0.52668256 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  143 , Train loss: 10.69094850196213 , Train accuracy: 0.7407338554439179 , argmax_recall_score: 0.5328217 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  144 , Train loss: 10.51878791558938 , Train accuracy: 0.7448087173956941 , argmax_recall_score: 0.53913635 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  145 , Train loss: 10.355999571378113 , Train accuracy: 0.7487512094155291 , argmax_recall_score: 0.54529154 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  146 , Train loss: 10.176855915882548 , Train accuracy: 0.7530036440792672 , argmax_recall_score: 0.5515718 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  147 , Train loss: 10.043722746802159 , Train accuracy: 0.756433658013972 , argmax_recall_score: 0.55740136 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  148 , Train loss: 9.905663756073498 , Train accuracy: 0.7595990670184661 , argmax_recall_score: 0.56246626 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  149 , Train loss: 9.811856144764384 , Train accuracy: 0.7613224398734642 , argmax_recall_score: 0.565258 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  150 , Train loss: 9.642525532206552 , Train accuracy: 0.7652330875467771 , argmax_recall_score: 0.571235 , teacher_prob: 0 , time:  11  sec \n",
            "Epoch:  151 , Train loss: 9.51729019352647 , Train accuracy: 0.7678374423443277 , argmax_recall_score: 0.5752919 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  152 , Train loss: 9.432106252576483 , Train accuracy: 0.7700581807132251 , argmax_recall_score: 0.5790571 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  153 , Train loss: 9.28963568953217 , Train accuracy: 0.7733191227572853 , argmax_recall_score: 0.58423966 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  154 , Train loss: 9.161875974936564 , Train accuracy: 0.7763487884880457 , argmax_recall_score: 0.58983076 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  155 , Train loss: 9.055267943710577 , Train accuracy: 0.778773951369574 , argmax_recall_score: 0.5944004 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  156 , Train loss: 8.929180676819849 , Train accuracy: 0.7816027008631676 , argmax_recall_score: 0.59864545 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  157 , Train loss: 8.84014675265453 , Train accuracy: 0.7833813965235642 , argmax_recall_score: 0.6020116 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  158 , Train loss: 8.705805622163366 , Train accuracy: 0.7863055441230523 , argmax_recall_score: 0.6070096 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  159 , Train loss: 7.410424779673091 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  160 , Train loss: 6.467419632145615 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  161 , Train loss: 5.723589451586614 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  162 , Train loss: 5.16120159430582 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  163 , Train loss: 4.751727127638019 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  164 , Train loss: 4.45889717633607 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  165 , Train loss: 4.243899587725029 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  166 , Train loss: 4.076689282401663 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  167 , Train loss: 3.935919116755001 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  168 , Train loss: 3.81577971333363 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  169 , Train loss: 3.7098221036254384 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  170 , Train loss: 3.6324868202209473 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  171 , Train loss: 3.558230142124364 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  172 , Train loss: 3.4897787258273265 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  173 , Train loss: 3.4289563640219267 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  174 , Train loss: 3.3766249750481276 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  175 , Train loss: 3.3153515291995688 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  176 , Train loss: 3.256627860616465 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  177 , Train loss: 3.2063317767909316 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  178 , Train loss: 3.1614625649374037 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  179 , Train loss: 3.1248119620026134 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  180 , Train loss: 3.0827569492527696 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  181 , Train loss: 3.032287480401211 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  182 , Train loss: 2.992083944258143 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  183 , Train loss: 2.9510616513549306 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  184 , Train loss: 2.9163380060039583 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  185 , Train loss: 2.885373568925701 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  186 , Train loss: 2.8423814968984633 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  187 , Train loss: 2.8050081847143953 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  188 , Train loss: 2.773303180444436 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  189 , Train loss: 2.7267628927699854 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  190 , Train loss: 2.7044547972131947 , Train accuracy: 0 , teacher_prob: 1 , time:  8  sec \n",
            "Epoch:  191 , Train loss: 2.676248867003644 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  192 , Train loss: 2.6476278070543633 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  193 , Train loss: 2.6189952873792803 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  194 , Train loss: 2.5856856205424323 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  195 , Train loss: 2.5487122652960603 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  196 , Train loss: 2.5205227312494496 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  197 , Train loss: 2.496924138460003 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  198 , Train loss: 2.4638263866549632 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  199 , Train loss: 17.09609513204606 , Train accuracy: 0.6274729602220298 , argmax_recall_score: 0.43221605 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  200 , Train loss: 14.548463649437076 , Train accuracy: 0.644619791487433 , argmax_recall_score: 0.42745772 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  201 , Train loss: 12.08251082310911 , Train accuracy: 0.6981802281997774 , argmax_recall_score: 0.49409184 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  202 , Train loss: 10.46649806225886 , Train accuracy: 0.7378113270484903 , argmax_recall_score: 0.5499032 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  203 , Train loss: 9.36412826913302 , Train accuracy: 0.7660955835763062 , argmax_recall_score: 0.5930044 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  204 , Train loss: 8.598285159126657 , Train accuracy: 0.7865669106451421 , argmax_recall_score: 0.6234516 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  205 , Train loss: 8.15057376173676 , Train accuracy: 0.797427991747906 , argmax_recall_score: 0.6414976 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  206 , Train loss: 7.822630163098945 , Train accuracy: 0.8055407438254116 , argmax_recall_score: 0.65492624 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  207 , Train loss: 7.565374022624532 , Train accuracy: 0.812034021975131 , argmax_recall_score: 0.66523194 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  208 , Train loss: 7.311368950077744 , Train accuracy: 0.8180884958244703 , argmax_recall_score: 0.6755153 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  209 , Train loss: 7.179318818889681 , Train accuracy: 0.8211701110188364 , argmax_recall_score: 0.68044454 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  210 , Train loss: 7.059182041981181 , Train accuracy: 0.8239604044159946 , argmax_recall_score: 0.68544424 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  211 , Train loss: 6.948220323343746 , Train accuracy: 0.8261745310349785 , argmax_recall_score: 0.68885785 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  212 , Train loss: 6.83076394972254 , Train accuracy: 0.828777131694797 , argmax_recall_score: 0.69288814 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  213 , Train loss: 6.734151371189805 , Train accuracy: 0.831461906960683 , argmax_recall_score: 0.6976224 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  214 , Train loss: 6.649997351599521 , Train accuracy: 0.8328553669883658 , argmax_recall_score: 0.69976085 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  215 , Train loss: 6.578930307607182 , Train accuracy: 0.8347310799587172 , argmax_recall_score: 0.7029235 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  216 , Train loss: 6.5197642044942885 , Train accuracy: 0.8358247173678747 , argmax_recall_score: 0.70526874 , teacher_prob: 0 , time:  11  sec \n",
            "Epoch:  217 , Train loss: 6.426325899655701 , Train accuracy: 0.8378128300867551 , argmax_recall_score: 0.7078853 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  218 , Train loss: 6.395401720140801 , Train accuracy: 0.8386384892239093 , argmax_recall_score: 0.70955455 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  219 , Train loss: 6.326261129535612 , Train accuracy: 0.8401390865869247 , argmax_recall_score: 0.7120109 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  220 , Train loss: 6.292388704956555 , Train accuracy: 0.8405896301167429 , argmax_recall_score: 0.7126974 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  221 , Train loss: 6.226677472474145 , Train accuracy: 0.8422811586252189 , argmax_recall_score: 0.7162718 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  222 , Train loss: 6.207851847664255 , Train accuracy: 0.8421857605193245 , argmax_recall_score: 0.71607953 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  223 , Train loss: 6.134357139712474 , Train accuracy: 0.8445689590289529 , argmax_recall_score: 0.71987706 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  224 , Train loss: 6.0929442077386575 , Train accuracy: 0.8445622123453678 , argmax_recall_score: 0.7203991 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  225 , Train loss: 6.025996114386887 , Train accuracy: 0.8462270239868465 , argmax_recall_score: 0.7230409 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  226 , Train loss: 5.974127769470215 , Train accuracy: 0.8478482520523649 , argmax_recall_score: 0.7251059 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  227 , Train loss: 5.955309883492892 , Train accuracy: 0.8480491682895315 , argmax_recall_score: 0.7260532 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  228 , Train loss: 5.887618143050397 , Train accuracy: 0.8491294472651902 , argmax_recall_score: 0.72852594 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  229 , Train loss: 5.8853940807405065 , Train accuracy: 0.8491612916117123 , argmax_recall_score: 0.72872764 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  230 , Train loss: 5.862560741236953 , Train accuracy: 0.8499401287647853 , argmax_recall_score: 0.729324 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  231 , Train loss: 5.81855272855915 , Train accuracy: 0.8507255776677718 , argmax_recall_score: 0.73104286 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  232 , Train loss: 5.766026543789223 , Train accuracy: 0.8515044148208448 , argmax_recall_score: 0.7315019 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  233 , Train loss: 5.708804724646396 , Train accuracy: 0.8533667693576976 , argmax_recall_score: 0.7354597 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  234 , Train loss: 5.670578479766846 , Train accuracy: 0.8538256787751615 , argmax_recall_score: 0.7367288 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  235 , Train loss: 5.594080784281746 , Train accuracy: 0.8555020947124066 , argmax_recall_score: 0.73929435 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  236 , Train loss: 5.5624357755066915 , Train accuracy: 0.8563245154414398 , argmax_recall_score: 0.7406664 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  237 , Train loss: 5.552074463641057 , Train accuracy: 0.8561252184083337 , argmax_recall_score: 0.74134225 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  238 , Train loss: 5.49523048713559 , Train accuracy: 0.8573779426164296 , argmax_recall_score: 0.74272346 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  239 , Train loss: 3.9312725614328854 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  240 , Train loss: 3.1723006748762286 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  241 , Train loss: 2.8000167002443406 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  242 , Train loss: 2.5376721718272224 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  243 , Train loss: 2.3663758176272034 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  244 , Train loss: 2.256546196390371 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  245 , Train loss: 2.1673246993393196 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  246 , Train loss: 2.101448893547058 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  247 , Train loss: 2.04887032118 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  248 , Train loss: 2.0107368875722416 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  249 , Train loss: 1.9787157617631506 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  250 , Train loss: 1.942783465150927 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  251 , Train loss: 1.9145169629425298 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  252 , Train loss: 1.8840933901364687 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  253 , Train loss: 1.8624960614032433 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  254 , Train loss: 1.8363954688681932 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  255 , Train loss: 1.8208890176210246 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  256 , Train loss: 1.7995243834667518 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  257 , Train loss: 1.7756275700741126 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  258 , Train loss: 1.7647034789695115 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  259 , Train loss: 1.7408297237802723 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  260 , Train loss: 1.7201175787409797 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  261 , Train loss: 1.7064908234799494 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  262 , Train loss: 1.6880310852019513 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  263 , Train loss: 1.6699585386964142 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  264 , Train loss: 1.656161304380073 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  265 , Train loss: 1.6417408380352083 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  266 , Train loss: 1.6339587031817826 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  267 , Train loss: 1.6162120690111255 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  268 , Train loss: 1.6008500818346367 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  269 , Train loss: 1.5869294365898508 , Train accuracy: 0 , teacher_prob: 1 , time:  8  sec \n",
            "Epoch:  270 , Train loss: 1.5750113999257322 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  271 , Train loss: 1.5557457658111071 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  272 , Train loss: 1.5368992246565272 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  273 , Train loss: 1.5340696866395043 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  274 , Train loss: 1.5241128401678117 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  275 , Train loss: 1.5055042485721777 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  276 , Train loss: 1.488533305340126 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  277 , Train loss: 1.4762554266413703 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  278 , Train loss: 1.47199667868067 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  279 , Train loss: 10.810554316786469 , Train accuracy: 0.7698690037054967 , argmax_recall_score: 0.6289167 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  280 , Train loss: 9.269426908649383 , Train accuracy: 0.7744143793705033 , argmax_recall_score: 0.6206159 , teacher_prob: 0 , time:  11  sec \n",
            "Epoch:  281 , Train loss: 7.6873595128293895 , Train accuracy: 0.8084381745579025 , argmax_recall_score: 0.67046034 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  282 , Train loss: 6.622390184246126 , Train accuracy: 0.8328368810753424 , argmax_recall_score: 0.71026605 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  283 , Train loss: 5.960496972818843 , Train accuracy: 0.8492399579423155 , argmax_recall_score: 0.73760223 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  284 , Train loss: 5.607509519233078 , Train accuracy: 0.8570898592273425 , argmax_recall_score: 0.75003403 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  285 , Train loss: 5.394217186286801 , Train accuracy: 0.8617524922530595 , argmax_recall_score: 0.75792027 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  286 , Train loss: 5.226274177676341 , Train accuracy: 0.8652328364473097 , argmax_recall_score: 0.76419044 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  287 , Train loss: 5.140436305374395 , Train accuracy: 0.866941096731077 , argmax_recall_score: 0.76667905 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  288 , Train loss: 5.069694151643847 , Train accuracy: 0.868225665285695 , argmax_recall_score: 0.7679892 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  289 , Train loss: 4.962798743951516 , Train accuracy: 0.8702773317639478 , argmax_recall_score: 0.77207094 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  290 , Train loss: 4.9003768279904225 , Train accuracy: 0.8717261145970291 , argmax_recall_score: 0.7736195 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  291 , Train loss: 4.857894006322642 , Train accuracy: 0.8729386285709574 , argmax_recall_score: 0.7765015 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  292 , Train loss: 4.813766299701128 , Train accuracy: 0.8731311789204783 , argmax_recall_score: 0.77632993 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  293 , Train loss: 4.764732939298035 , Train accuracy: 0.8746335304212259 , argmax_recall_score: 0.779383 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  294 , Train loss: 4.753130709538694 , Train accuracy: 0.8740909621273059 , argmax_recall_score: 0.7781477 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  295 , Train loss: 4.705209192682485 , Train accuracy: 0.8753838965695695 , argmax_recall_score: 0.7802648 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  296 , Train loss: 4.676635226265329 , Train accuracy: 0.8758376785075086 , argmax_recall_score: 0.7808068 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  297 , Train loss: 4.658728099260174 , Train accuracy: 0.8762246682779544 , argmax_recall_score: 0.7815248 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  298 , Train loss: 4.612672844871145 , Train accuracy: 0.8768141934896274 , argmax_recall_score: 0.7833399 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  299 , Train loss: 4.579414723349399 , Train accuracy: 0.8774505406853814 , argmax_recall_score: 0.78338623 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  300 , Train loss: 4.567784641609817 , Train accuracy: 0.8780585518100777 , argmax_recall_score: 0.7842707 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  301 , Train loss: 4.546286027939593 , Train accuracy: 0.8783649861785165 , argmax_recall_score: 0.7848065 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  302 , Train loss: 4.52913257723949 , Train accuracy: 0.8789228019773391 , argmax_recall_score: 0.7862797 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  303 , Train loss: 4.500799229887665 , Train accuracy: 0.8791304648980909 , argmax_recall_score: 0.786321 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  304 , Train loss: 4.490092027382772 , Train accuracy: 0.8794319067206766 , argmax_recall_score: 0.78657067 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  305 , Train loss: 4.447879525481677 , Train accuracy: 0.880274297633122 , argmax_recall_score: 0.78785956 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  306 , Train loss: 4.4199150702992425 , Train accuracy: 0.8808889205077318 , argmax_recall_score: 0.7890472 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  307 , Train loss: 4.416800983616563 , Train accuracy: 0.8811301819127378 , argmax_recall_score: 0.7898588 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  308 , Train loss: 4.39243839217014 , Train accuracy: 0.8809878268890906 , argmax_recall_score: 0.7893677 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  309 , Train loss: 4.387281894683838 , Train accuracy: 0.8812875145739442 , argmax_recall_score: 0.78993225 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  310 , Train loss: 4.369385742750324 , Train accuracy: 0.8816811160943036 , argmax_recall_score: 0.7907107 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  311 , Train loss: 4.356401240239378 , Train accuracy: 0.8820512391557864 , argmax_recall_score: 0.7906948 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  312 , Train loss: 4.3481295069710155 , Train accuracy: 0.8819524677080994 , argmax_recall_score: 0.79081655 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  313 , Train loss: 4.311555389498101 , Train accuracy: 0.882592188245646 , argmax_recall_score: 0.7918137 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  314 , Train loss: 4.277252580298752 , Train accuracy: 0.883526738855865 , argmax_recall_score: 0.79335237 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  315 , Train loss: 4.27517147142379 , Train accuracy: 0.8832454021503631 , argmax_recall_score: 0.792778 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  316 , Train loss: 4.26116427828054 , Train accuracy: 0.8838199497244769 , argmax_recall_score: 0.793729 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  317 , Train loss: 4.240165749534231 , Train accuracy: 0.8840527103081655 , argmax_recall_score: 0.7936893 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  318 , Train loss: 4.226410041089918 , Train accuracy: 0.8844596702620234 , argmax_recall_score: 0.79492164 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  319 , Train loss: 2.5819506762457674 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  320 , Train loss: 1.8988993167877197 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  321 , Train loss: 1.6666540650070691 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  322 , Train loss: 1.5312124862045537 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  323 , Train loss: 1.4564914078008933 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  324 , Train loss: 1.4013858290969348 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  325 , Train loss: 1.3605274001105887 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  326 , Train loss: 1.331071533140589 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  327 , Train loss: 1.3130542782486463 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  328 , Train loss: 1.296504391998541 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  329 , Train loss: 1.2776587439365075 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  330 , Train loss: 1.259020269894209 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  331 , Train loss: 1.2468859015918168 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  332 , Train loss: 1.2341911910010166 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  333 , Train loss: 1.224847351918455 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  334 , Train loss: 1.2084702429224232 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  335 , Train loss: 1.1970800884434434 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  336 , Train loss: 1.189575545123366 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  337 , Train loss: 1.1793751736156275 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  338 , Train loss: 1.170098267617773 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  339 , Train loss: 1.1681635966066455 , Train accuracy: 0 , teacher_prob: 1 , time:  6  sec \n",
            "Epoch:  340 , Train loss: 1.1572589991522617 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  341 , Train loss: 1.1471665843588408 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  342 , Train loss: 1.1358899229862651 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  343 , Train loss: 1.130621871987327 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  344 , Train loss: 1.1222421126287492 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  345 , Train loss: 1.115878773517296 , Train accuracy: 0 , teacher_prob: 1 , time:  10  sec \n",
            "Epoch:  346 , Train loss: 1.1102070378475501 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  347 , Train loss: 1.1026824556413244 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  348 , Train loss: 1.0961327924103033 , Train accuracy: 0 , teacher_prob: 1 , time:  8  sec \n",
            "Epoch:  349 , Train loss: 1.0849681643188978 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  350 , Train loss: 1.0793411936916288 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  351 , Train loss: 1.0746758404325267 , Train accuracy: 0 , teacher_prob: 1 , time:  8  sec \n",
            "Epoch:  352 , Train loss: 1.0723840281611583 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  353 , Train loss: 1.0636741704628117 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  354 , Train loss: 1.060743627978153 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  355 , Train loss: 1.05378209958311 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  356 , Train loss: 1.04774059917106 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  357 , Train loss: 1.0428528893189353 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  358 , Train loss: 1.0349956350248368 , Train accuracy: 0 , teacher_prob: 1 , time:  7  sec \n",
            "Epoch:  359 , Train loss: 7.570725386259986 , Train accuracy: 0.8436762378569621 , argmax_recall_score: 0.7443575 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  360 , Train loss: 6.205171092611844 , Train accuracy: 0.8482886755568053 , argmax_recall_score: 0.742003 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  361 , Train loss: 5.343133582443487 , Train accuracy: 0.8646851406738649 , argmax_recall_score: 0.7679068 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  362 , Train loss: 4.809151758913134 , Train accuracy: 0.8751912112863768 , argmax_recall_score: 0.7852503 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  363 , Train loss: 4.512619155352233 , Train accuracy: 0.8811318011167982 , argmax_recall_score: 0.7951614 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  364 , Train loss: 4.3611187739450425 , Train accuracy: 0.8837612535772857 , argmax_recall_score: 0.7992395 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  365 , Train loss: 4.267625367055174 , Train accuracy: 0.8852184022980126 , argmax_recall_score: 0.80138135 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  366 , Train loss: 4.185224232126455 , Train accuracy: 0.8868227636545681 , argmax_recall_score: 0.80366325 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  367 , Train loss: 4.123683339259664 , Train accuracy: 0.8876250792665176 , argmax_recall_score: 0.8047998 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  368 , Train loss: 4.098281532037453 , Train accuracy: 0.8880153074450843 , argmax_recall_score: 0.8055987 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  369 , Train loss: 4.0370975791430865 , Train accuracy: 0.8889582239429489 , argmax_recall_score: 0.80628675 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  370 , Train loss: 4.006026002227283 , Train accuracy: 0.889532636583391 , argmax_recall_score: 0.80678016 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  371 , Train loss: 3.9730389313619643 , Train accuracy: 0.8901137959074182 , argmax_recall_score: 0.80739963 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  372 , Train loss: 3.949420968040091 , Train accuracy: 0.8908625428517014 , argmax_recall_score: 0.80885303 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  373 , Train loss: 3.946952616582151 , Train accuracy: 0.8903700349499833 , argmax_recall_score: 0.8076707 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  374 , Train loss: 3.929355719050423 , Train accuracy: 0.8909931586459104 , argmax_recall_score: 0.80878955 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  375 , Train loss: 3.916419244203411 , Train accuracy: 0.8909880311663857 , argmax_recall_score: 0.8084616 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  376 , Train loss: 3.8964517702821824 , Train accuracy: 0.8909629335034489 , argmax_recall_score: 0.80881315 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  377 , Train loss: 3.8785258981048085 , Train accuracy: 0.8916798361012099 , argmax_recall_score: 0.80932313 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  378 , Train loss: 3.8487081058689805 , Train accuracy: 0.8918522813336471 , argmax_recall_score: 0.8100062 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  379 , Train loss: 3.8355285574178226 , Train accuracy: 0.8921453572685872 , argmax_recall_score: 0.81047493 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  380 , Train loss: 3.826020490927774 , Train accuracy: 0.8922860930881741 , argmax_recall_score: 0.81039083 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  381 , Train loss: 3.822947443508711 , Train accuracy: 0.8923898570817141 , argmax_recall_score: 0.8101718 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  382 , Train loss: 3.8080367260291927 , Train accuracy: 0.8928471472951175 , argmax_recall_score: 0.8108448 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  383 , Train loss: 3.799054849343222 , Train accuracy: 0.8927416291638454 , argmax_recall_score: 0.81156105 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  384 , Train loss: 3.781865487333204 , Train accuracy: 0.8932272554483065 , argmax_recall_score: 0.81141174 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  385 , Train loss: 3.76040952322913 , Train accuracy: 0.8933361469213713 , argmax_recall_score: 0.8118692 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  386 , Train loss: 3.7623862047664454 , Train accuracy: 0.8934685168533125 , argmax_recall_score: 0.8118608 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  387 , Train loss: 3.745527763835719 , Train accuracy: 0.8937247558958776 , argmax_recall_score: 0.8126644 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  388 , Train loss: 3.7303607151156566 , Train accuracy: 0.893783317109397 , argmax_recall_score: 0.8126993 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  389 , Train loss: 3.725424446043421 , Train accuracy: 0.8939507697959811 , argmax_recall_score: 0.8127556 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  390 , Train loss: 3.7146467029071246 , Train accuracy: 0.8939692557090044 , argmax_recall_score: 0.8128625 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  391 , Train loss: 3.7014105906252 , Train accuracy: 0.8941132299367122 , argmax_recall_score: 0.81307864 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  392 , Train loss: 3.6960481971990866 , Train accuracy: 0.8945068314570714 , argmax_recall_score: 0.8133863 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  393 , Train loss: 3.688889507387505 , Train accuracy: 0.8946441939348657 , argmax_recall_score: 0.81389636 , teacher_prob: 0 , time:  9  sec \n",
            "Epoch:  394 , Train loss: 3.6811005365653116 , Train accuracy: 0.8945253173700949 , argmax_recall_score: 0.81342256 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  395 , Train loss: 3.669192697181076 , Train accuracy: 0.8947580779537836 , argmax_recall_score: 0.8138923 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  396 , Train loss: 3.6677936686844124 , Train accuracy: 0.8947296069490541 , argmax_recall_score: 0.81411177 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  397 , Train loss: 3.6522001243028486 , Train accuracy: 0.8953426106196034 , argmax_recall_score: 0.8145097 , teacher_prob: 0 , time:  8  sec \n",
            "Epoch:  398 , Train loss: 3.63929132946202 , Train accuracy: 0.8955050707603345 , argmax_recall_score: 0.8142952 , teacher_prob: 0 , time:  8  sec \n",
            "distribution correction training...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vueXnphmBoiu",
        "colab_type": "text"
      },
      "source": [
        "Above is the main training circle of seq2seq chatbot model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9-VAQj_etWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "print(\"seq2seq is trained\")\n",
        "\n",
        "print(\"training loss: \")\n",
        "plt.plot(train_losses)\n",
        "plt.show()\n",
        "plt.close()\n",
        "print(\"training accuracy: \")\n",
        "plt.plot(train_accuracies)\n",
        "plt.show()\n",
        "plt.close()\n",
        "print(\"argmax recall: \")\n",
        "plt.plot(correction_argmax_recall)\n",
        "plt.show()\n",
        "plt.close()\n",
        "print(\"sample recall: \")\n",
        "plt.plot(correction_sample_recall)\n",
        "plt.show()\n",
        "plt.close()\n",
        "\n",
        "\n",
        "\n",
        "common_questions = [\"are you a human ?\", \n",
        "                    \"what is important to you ?\",\n",
        "                    \"hello ! how are you ?\",\n",
        "                    \"hello !\",\n",
        "                    \"what is your business here ?\",\n",
        "                    \"tell me something about humans\",\n",
        "                    \"what is your purpose ?\",\n",
        "                    \"are you good guy ?\", \"how can you prove it ?\",\n",
        "                    \"do you want to harm humanity ?\",\"why ?\",\n",
        "                    \"what is on your mind ?\",\n",
        "                    \"can you be my friend ?\",\"are you a robot ?\",\n",
        "                    \"say something about cows\",\n",
        "                    \"say about yourself\",\n",
        "                    \"will artificial intelligence rule the world ?\",\n",
        "                    \"what is your favorite movie ?\",\n",
        "                    \"are you smart enough ?\",\n",
        "                    \"it is bad weather today\",\n",
        "                    \"how can you explain your existence ?\"]\n",
        "\n",
        "comedy_questions = [\n",
        "                    \"hi , want to have some good time ?\",\n",
        "                    \"you are funny , are not you ?\",\n",
        "                    \"what is your mission in this life ?\", \n",
        "                    \"i want a joke .\",\n",
        "                    \"make me laugh please\"]\n",
        "\n",
        "romance_questions = [\"hello , nice to meet you !\",\n",
        "                     \"hi , how are you ?\",\n",
        "                     \"hi pretty one !\",\n",
        "                     \"what is on your mind right now ?\",\n",
        "                     \"what do you want from me ?\",\n",
        "                     \"do you want something from me ?\",\n",
        "                     \"what is your name swettie ?\",\n",
        "                     \"do you like to play ?\",\n",
        "                     \"would you play with me ?\",\n",
        "                     \"am i handsome ?\",\n",
        "                     \"what is on your mind ?\"]\n",
        "\n",
        "\n",
        "\n",
        "def talk_to_me(test_phrases, sampling = False):\n",
        "    converted_test_phrases = []\n",
        "    converted_test_answers = []\n",
        "    test_answers = []\n",
        "\n",
        "    with tf.device(\"cpu:0\"):\n",
        "\n",
        "        for phrase in test_phrases:\n",
        "            phrase = phrase.strip(\" \")\n",
        "            phrase_words = phrase.split(\" \")\n",
        "            skip = False\n",
        "            phrase_words = [\"BEGIN\"] + phrase_words + [\"END\"]\n",
        "            if not skip:\n",
        "                predicted = seq2seq.predict(phrase_words, sampling = sampling)\n",
        "                test_answers.append(predicted)\n",
        "\n",
        "    for i in range(len(test_phrases)):\n",
        "        phrase = test_phrases[i].strip(\" \").split(\" \")\n",
        "\n",
        "        answer = test_answers[i]\n",
        "        converted_answer = \"\"\n",
        "        for word in answer:\n",
        "            converted_answer += word\n",
        "            converted_answer += \" \"\n",
        "\n",
        "        converted_test_answers.append(converted_answer)  \n",
        "\n",
        "    for i in range(len(test_phrases)):\n",
        "        print(test_phrases[i])\n",
        "        print(converted_test_answers[i])\n",
        "        print(\"\\n\")  \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMDDmhtZex5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "talk_to_me(romance_questions, sampling = True) #old"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "88hFOyaXi1lk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "talk_to_me(comedy_questions, sampling = True) #new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-Ih9Pm7bVN-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "talk_to_me(common_questions, sampling = False) #new"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wQNI__MLzWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "talk_to_me([\"who are you ?\"], sampling = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhnBSIEjAHUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files\n",
        "import h5py\n",
        "import pickle\n",
        "\n",
        "def download_weights(weights):\n",
        "    weight_file_name = \"weights2_final_v2\" + \".\" + str(MIN_TOKEN_FREQ) + \".\" + str(MAX_TOKENS_X) + \".\" + str(MAX_TOKENS_Y) + \".\" + \"all_except.\" * int(WITH_EXCEPTION) + GENRES + \".h5\"\n",
        "    with h5py.File(weight_file_name, \"w\") as h5f:\n",
        "        i = 0\n",
        "        for weight in weights:\n",
        "            group = h5f.create_group(str(i))\n",
        "            i += 1\n",
        "            j = 0\n",
        "            for kernel in weight:\n",
        "                group.create_dataset(str(j), data=kernel)\n",
        "                j += 1\n",
        "    h5f.close()\n",
        "    files.download(weight_file_name)\n",
        "\n",
        "with tf.device(\"cpu:0\"):\n",
        "    weights = seq2seq.save()\n",
        "\n",
        "\n",
        "words_file_name = \"words2_final_v2\" + \".\" + str(MIN_TOKEN_FREQ) + \".\" + str(MAX_TOKENS_X) + \".\" + str(MAX_TOKENS_Y) + \".\" + \"all_except.\" * int(WITH_EXCEPTION) + GENRES + \".dict\"\n",
        "with open(words_file_name, \"wb\") as dict_file:\n",
        "    pickle.dump(word_dict, dict_file)\n",
        "\n",
        "download_weights(weights)\n",
        "files.download(words_file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lM9YYYtCB8VX",
        "colab_type": "text"
      },
      "source": [
        "And last step is saving weights and words dictionary tokens of trained seq2seq model, code above saves weights in fucntion and two arrays of word indices:\n",
        "first array is array of indices of words that saved seq2seq model is trained on in twitter 50-d embeddings array with 3 adititional words (\"EMPTY\" with index 0, \"START\" with index 1 and \"END\" with index 2), all 3 words are added at the begining of embeddings array.\n",
        "second array is array of word tokens in seq2seq2 model dictionary with which seq2seq model operates, every token at index i in this array corresponds to word index at the same index in first array, so recreated dictionary of saved model will have as keys words from embeddings file with 3 additional words at the begining at indices from first array and as values will have tokens from second array."
      ]
    }
  ]
}